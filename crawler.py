"""
é‡‘èç›‘ç®¡æ€»å±€è¡Œæ”¿å¤„ç½šä¿¡æ¯çˆ¬è™«
"""

import time
import logging
import re
import os
from typing import List, Dict, Optional
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import urllib.parse
import random

from config import BASE_URLS, SELENIUM_CONFIG, CRAWL_CONFIG, WEBDRIVER_CONFIG
from utils import setup_logging, clean_text, format_date, get_current_timestamp


class NFRACrawler:
    """é‡‘èç›‘ç®¡æ€»å±€è¡Œæ”¿å¤„ç½šä¿¡æ¯çˆ¬è™«"""
    
    def __init__(self, headless: bool = True):
        self.logger = setup_logging()
        self.driver = None
        self.wait = None
        self.headless = headless  # æ·»åŠ headlesså±æ€§
        self.driver_path = None  # ç¼“å­˜driverè·¯å¾„
        
    def _get_driver_path(self):
        """è·å–ChromeDriverè·¯å¾„ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°driver"""
        # å¦‚æœå·²ç»ç¼“å­˜äº†æœ‰æ•ˆçš„driverè·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if self.driver_path and os.path.exists(self.driver_path):
            self.logger.info(f"ä½¿ç”¨å·²ç¼“å­˜çš„ChromeDriver: {self.driver_path}")
            return self.driver_path
        
        # 1. ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°driversç›®å½•
        if WEBDRIVER_CONFIG['use_local_driver']:
            local_driver_path = self._get_local_driver_path()
            if local_driver_path:
                self.driver_path = local_driver_path
                return local_driver_path
        
        # 2. å¦‚æœæœ¬åœ°æ²¡æœ‰ä¸”å…è®¸è‡ªåŠ¨ä¸‹è½½ï¼Œä½¿ç”¨webdriver_manager
        if WEBDRIVER_CONFIG['auto_download']:
            return self._download_driver_with_manager()
        
        # 3. æœ€åå°è¯•ç³»ç»Ÿè·¯å¾„
        return self._get_system_driver()
    
    def _get_local_driver_path(self):
        """æ£€æŸ¥æœ¬åœ°driversç›®å½•ä¸­çš„ChromeDriver"""
        try:
            local_dir = os.path.join(os.getcwd(), WEBDRIVER_CONFIG['local_driver_dir'])
            driver_file = WEBDRIVER_CONFIG['driver_filename']
            local_driver_path = os.path.join(local_dir, driver_file)
            
            if os.path.exists(local_driver_path):
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯æ‰§è¡Œ
                if os.access(local_driver_path, os.X_OK) or os.name == 'nt':  # Windowsä¸éœ€è¦æ‰§è¡Œæƒé™æ£€æŸ¥
                    self.logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°ChromeDriver: {local_driver_path}")
                    return local_driver_path
                else:
                    self.logger.warning(f"æœ¬åœ°ChromeDriveræ— æ‰§è¡Œæƒé™: {local_driver_path}")
            else:
                self.logger.info(f"æœ¬åœ°ChromeDriverä¸å­˜åœ¨: {local_driver_path}")
                
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥æœ¬åœ°ChromeDriverå¤±è´¥: {e}")
        
        return None
    
    def _download_driver_with_manager(self):
        """ä½¿ç”¨webdriver_managerä¸‹è½½ChromeDriver"""
        try:
            self.logger.info("â¬ æœ¬åœ°driverä¸å­˜åœ¨ï¼Œä½¿ç”¨webdriver_managerä¸‹è½½...")
            # ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œwebdriver_managerä¼šè‡ªåŠ¨å¤„ç†ç¼“å­˜
            manager = ChromeDriverManager()
            driver_path = manager.install()
            self.driver_path = driver_path
            self.logger.info(f"âœ… ChromeDriverå·²ä¸‹è½½å¹¶ç¼“å­˜è‡³: {driver_path}")
            
            # å°è¯•å¤åˆ¶åˆ°æœ¬åœ°driversç›®å½•ä»¥å¤‡åç”¨
            self._copy_to_local_drivers(driver_path)
            
            return driver_path
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½ChromeDriverå¤±è´¥: {e}")
            return None
    
    def _copy_to_local_drivers(self, source_path):
        """å°†ä¸‹è½½çš„driverå¤åˆ¶åˆ°æœ¬åœ°driversç›®å½•"""
        try:
            local_dir = os.path.join(os.getcwd(), WEBDRIVER_CONFIG['local_driver_dir'])
            os.makedirs(local_dir, exist_ok=True)
            
            driver_file = WEBDRIVER_CONFIG['driver_filename']
            local_path = os.path.join(local_dir, driver_file)
            
            import shutil
            shutil.copy2(source_path, local_path)
            self.logger.info(f"ğŸ“¥ å·²å¤åˆ¶ChromeDriveråˆ°æœ¬åœ°ç›®å½•: {local_path}")
            
        except Exception as e:
            self.logger.warning(f"å¤åˆ¶ChromeDriveråˆ°æœ¬åœ°ç›®å½•å¤±è´¥: {e}")
    
    def _get_system_driver(self):
        """å°è¯•ä»ç³»ç»Ÿè·¯å¾„è·å–ChromeDriver"""
        try:
            import shutil
            system_driver = shutil.which('chromedriver')
            if system_driver:
                self.logger.info(f"âœ… ä½¿ç”¨ç³»ç»ŸChromeDriver: {system_driver}")
                return system_driver
        except Exception as e:
            self.logger.warning(f"æŸ¥æ‰¾ç³»ç»ŸChromeDriverå¤±è´¥: {e}")
        
        self.logger.warning("âŒ æ— æ³•è·å–ChromeDriverï¼Œå°†å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®")
        return None
        
    def _setup_chrome_options(self):
        """é…ç½®Chromeé€‰é¡¹ä»¥ç»•è¿‡åçˆ¬è™«æ£€æµ‹"""
        options = Options()
        
        # åŸºç¡€é…ç½®
        if self.headless:
            options.add_argument('--headless')
        
        # å¢å¼ºåæ£€æµ‹é…ç½®
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-web-security')
        options.add_argument('--disable-features=VizDisplayCompositor')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        # ç§»é™¤ç¦ç”¨å›¾ç‰‡å’ŒJavaScriptçš„é…ç½® - è¿™äº›ä¼šå¯¼è‡´ç°ä»£ç½‘ç«™æ— æ³•æ­£å¸¸åŠ è½½
        # options.add_argument('--disable-images')  # ç¦ç”¨å›¾ç‰‡åŠ è½½åŠ é€Ÿ
        # options.add_argument('--disable-javascript')  # æš‚æ—¶ç¦ç”¨JS
        
        # SSL å’Œç½‘ç»œé…ç½®
        options.add_argument('--ignore-ssl-errors=yes')
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--ignore-certificate-errors-spki-list')
        options.add_argument('--allow-running-insecure-content')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-renderer-backgrounding')
        
        # ç”¨æˆ·ä»£ç†é…ç½®
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # çª—å£å¤§å°
        options.add_argument('--window-size=1920,1080')
        
        # å®éªŒæ€§é€‰é¡¹
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # ç¦ç”¨blinkç‰¹æ€§
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        return options
    
    def setup_driver(self) -> bool:
        """åˆå§‹åŒ–Chrome WebDriver"""
        try:
            chrome_options = self._setup_chrome_options()
            
            # è·å–ChromeDriverè·¯å¾„
            driver_path = self._get_driver_path()
            
            # åˆ›å»ºServiceå¯¹è±¡
            if driver_path:
                service = Service(driver_path)
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œè®©seleniumè‡ªåŠ¨æŸ¥æ‰¾
                service = Service()
            
            # åˆ›å»ºdriverå®ä¾‹
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ä¼˜åŒ–åçš„è¶…æ—¶é…ç½®
            self.driver.set_page_load_timeout(SELENIUM_CONFIG['page_load_timeout'])  # ä½¿ç”¨é…ç½®ä¸­çš„20ç§’
            self.driver.implicitly_wait(SELENIUM_CONFIG['implicit_wait'])  # ä½¿ç”¨é…ç½®ä¸­çš„5ç§’
            self.driver.set_script_timeout(30)  # å‡å°‘è„šæœ¬æ‰§è¡Œè¶…æ—¶åˆ°30ç§’
            
            # éšè—WebDriverç‰¹å¾
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # å‡å°‘WebDriverWaitç­‰å¾…æ—¶é—´
            self.wait = WebDriverWait(self.driver, 20)  # ä»45ç§’å‡å°‘åˆ°20ç§’
            
            self.logger.info("Chrome WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–WebDriverå¤±è´¥: {e}")
            return False
    
    def close_driver(self):
        """å…³é—­WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("WebDriver å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­WebDriverå¤±è´¥: {e}")
    
    def load_page_with_retry(self, url: str, max_retries: int = 3) -> bool:
        """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢åŠ è½½"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"æ­£åœ¨åŠ è½½é¡µé¢: {url} (å°è¯• {attempt + 1}/{max_retries})")
                
                self.driver.get(url)
                
                # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å®Œæˆ - å’Œdebug_test.pyä¿æŒä¸€è‡´
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                
                # å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ï¼Œä½¿ç”¨éšæœºæ•°é¿å…è¢«æ£€æµ‹
                time.sleep(random.uniform(0.5, 1.5))  # ä»2-4ç§’å‡å°‘åˆ°0.5-1.5ç§’
                
                self.logger.info("é¡µé¢åŠ è½½æˆåŠŸ")
                return True
                
            except TimeoutException:
                self.logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    # å‡å°‘é‡è¯•ç­‰å¾…æ—¶é—´
                    time.sleep(random.uniform(1, 2))  # ä»3-6ç§’å‡å°‘åˆ°1-2ç§’
                    continue
            except Exception as e:
                self.logger.error(f"é¡µé¢åŠ è½½å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(1, 2))  # ä»3-6ç§’å‡å°‘åˆ°1-2ç§’
                    continue
        
        self.logger.error(f"æ— æ³•åŠ è½½ {url} é¡µé¢")
        return False
    
    def get_page_publish_dates(self) -> List[str]:
        """è·å–å½“å‰é¡µé¢æ‰€æœ‰è®°å½•çš„å‘å¸ƒæ—¶é—´"""
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ—¶é—´æ ¼å¼
            time_elements = []
            
            # æ–¹å¼1ï¼šæŸ¥æ‰¾å¸¸è§çš„æ—¶é—´æ ¼å¼å…ƒç´  
            time_patterns = [
                '//td[contains(text(), "2024") or contains(text(), "2025")]',
                '//span[contains(text(), "2024") or contains(text(), "2025")]',
                '//div[contains(text(), "2024") or contains(text(), "2025")]'
            ]
            
            for pattern in time_patterns:
                elements = self.driver.find_elements(By.XPATH, pattern)
                time_elements.extend(elements)
            
            # æå–å’Œæ¸…ç†æ—¶é—´æ–‡æœ¬
            publish_dates = []
            for element in time_elements:
                text = clean_text(element.text)
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¥æœŸæ ¼å¼
                import re
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', text)
                if date_match:
                    publish_dates.append(date_match.group(1))
            
            # å»é‡å¹¶æ’åº
            unique_dates = list(set(publish_dates))
            unique_dates.sort(reverse=True)  # é™åºæ’åˆ—ï¼Œæœ€æ–°çš„åœ¨å‰
            
            self.logger.debug(f"å½“å‰é¡µé¢å‘ç°çš„å‘å¸ƒæ—¶é—´: {unique_dates}")
            return unique_dates
            
        except Exception as e:
            self.logger.warning(f"è·å–é¡µé¢å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return []

    def get_punishment_list_smart(self, category: str, target_year: int = None, target_month: int = None, max_pages: int = 10, use_smart_check: bool = False) -> List[Dict]:
        """æ™ºèƒ½è·å–å¤„ç½šä¿¡æ¯åˆ—è¡¨ - æ”¯æŒæŒ‰æœˆä»½è¿‡æ»¤"""
        url = BASE_URLS.get(category)
        if not url:
            self.logger.error(f"æœªæ‰¾åˆ°ç±»åˆ« '{category}' å¯¹åº”çš„URL")
            return []
        
        if not self.load_page_with_retry(url):
            self.logger.error(f"æ— æ³•åŠ è½½ {category} é¡µé¢")
            return []
        
        all_punishment_list = []
        current_page = 1
        found_target_month = False
        previous_page_latest_date = None  # è®°å½•å‰ä¸€é¡µçš„æœ€æ–°æ—¥æœŸ
        
        # å¦‚æœå¯ç”¨æ™ºèƒ½æ£€æŸ¥
        if use_smart_check and target_year is not None and target_month is not None:
            self.logger.info(f"å¯ç”¨æ™ºèƒ½ç²¾ç¡®æ—¥æœŸè¿‡æ»¤: {target_year}å¹´{target_month}æœˆ")
            target_month_str = f"{target_year}-{target_month:02d}"
            # æ„å»ºç²¾ç¡®çš„æ—¥æœŸèŒƒå›´
            from datetime import datetime
            target_start = datetime(target_year, target_month, 1)
            target_end = datetime(target_year, target_month + 1, 1) if target_month < 12 else datetime(target_year + 1, 1, 1)
        else:
            use_smart_check = False
        
        try:
            while current_page <= max_pages:
                self.logger.info(f"æ­£åœ¨è§£æ {category} ç¬¬ {current_page} é¡µ")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œä¼˜åŒ–ç­‰å¾…æ—¶é—´
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(0.5)  # å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ä»2ç§’åˆ°0.5ç§’
                
                # æ™ºèƒ½æ£€æŸ¥ï¼šå¦‚æœæŒ‡å®šäº†ç›®æ ‡æœˆä»½ï¼Œå…ˆæ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦åŒ…å«ç›®æ ‡æœˆä»½çš„æ•°æ®
                if use_smart_check:
                    # è·å–å½“å‰é¡µé¢çš„å‘å¸ƒæ—¶é—´
                    publish_dates = self.get_page_publish_dates()
                    current_page_latest_date = publish_dates[0] if publish_dates else None
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æœˆä»½
                    page_has_target = any(date.startswith(target_month_str) for date in publish_dates)
                    
                    if page_has_target:
                        found_target_month = True
                        self.logger.info(f"ç¬¬ {current_page} é¡µåŒ…å«ç›®æ ‡æœˆä»½ {target_month_str} æ•°æ®ï¼Œç»§ç»­çˆ¬å–")
                    else:
                        # æ²¡æœ‰ç›®æ ‡æœˆä»½æ•°æ®çš„å¤„ç†é€»è¾‘
                        if found_target_month:
                            # ä¹‹å‰æ‰¾åˆ°è¿‡ç›®æ ‡æœˆä»½ï¼Œç°åœ¨æ²¡æœ‰äº†ï¼Œè¯´æ˜å·²ç»è¶…è¿‡ç›®æ ‡æœˆä»½èŒƒå›´ï¼ˆé™åºæ’åˆ—ï¼‰
                            self.logger.info(f"ç¬¬ {current_page} é¡µä¸å†åŒ…å«ç›®æ ‡æœˆä»½æ•°æ®ï¼Œç›®æ ‡æœˆä»½èŒƒå›´å·²ç»“æŸï¼Œåœæ­¢çˆ¬å–")
                            break
                        elif current_page == 1:
                            # ç¬¬1é¡µæ²¡æœ‰ç›®æ ‡æœˆä»½æ•°æ®
                            if current_page_latest_date and current_page_latest_date > target_month_str:
                                self.logger.info(f"ç¬¬1é¡µæœ€æ–°æ•°æ®({current_page_latest_date})æ¯”ç›®æ ‡æœˆä»½({target_month_str})æ–°ï¼Œè¯¥åˆ†ç±»æ— ç›®æ ‡æœˆä»½æ•°æ®")
                                break
                            else:
                                self.logger.info(f"ç¬¬1é¡µæš‚æ— ç›®æ ‡æœˆä»½æ•°æ®ï¼Œæ£€æŸ¥ç¬¬2é¡µç¡®è®¤")
                        elif current_page == 2:
                            # ç¬¬2é¡µçš„å®‰å…¨æ£€æŸ¥
                            if (previous_page_latest_date and current_page_latest_date and 
                                current_page_latest_date < previous_page_latest_date):
                                self.logger.info(f"ç¬¬2é¡µæ•°æ®({current_page_latest_date})æ¯”ç¬¬1é¡µ({previous_page_latest_date})æ›´è€ä¸”æ— ç›®æ ‡æœˆä»½ï¼Œåœæ­¢ç¿»é¡µ")
                                break
                            else:
                                self.logger.info(f"ç¬¬2é¡µæš‚æ— ç›®æ ‡æœˆä»½æ•°æ®ï¼Œç»§ç»­æŸ¥æ‰¾")
                        else:
                            # ç¬¬3é¡µåŠä»¥åï¼Œå¦‚æœè¿˜æ²¡æ‰¾åˆ°ç›®æ ‡æœˆä»½å°±åœæ­¢
                            self.logger.info(f"ç¬¬{current_page}é¡µä»æ— ç›®æ ‡æœˆä»½æ•°æ®ï¼Œåœæ­¢æŸ¥æ‰¾")
                            break
                        
                        # è®°å½•å½“å‰é¡µæœ€æ–°æ—¥æœŸï¼Œç”¨äºä¸‹ä¸€é¡µæ¯”è¾ƒ
                        previous_page_latest_date = current_page_latest_date
                        
                        # æ²¡æœ‰ç›®æ ‡æœˆä»½æ•°æ®ï¼Œç¿»åˆ°ä¸‹ä¸€é¡µç»§ç»­æ£€æŸ¥
                        if current_page < max_pages:
                            try:
                                next_buttons = self.driver.find_elements(
                                    By.XPATH, 
                                    '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                                )
                                
                                if next_buttons:
                                    next_button = next_buttons[0]
                                    if next_button.is_enabled():
                                        self.driver.execute_script("arguments[0].click();", next_button)
                                        current_page += 1
                                        time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                        continue
                                    else:
                                        self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                        break
                                else:
                                    self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                                    break
                            except Exception as e:
                                self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                                break
                        else:
                            break
                
                # è§£æå½“å‰é¡µé¢çš„å¤„ç½šä¿¡æ¯ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ™ºèƒ½å¤„ç†é€»è¾‘ï¼‰
                try:
                    # æŸ¥æ‰¾åŒ…å«"è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€è¡¨"çš„é“¾æ¥
                    punishment_links = self.driver.find_elements(
                        By.XPATH, 
                        '//a[contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬ç¤º") or contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€") or contains(text(), "å¤„ç½šä¿¡æ¯")]'
                    )
                    
                    if not punishment_links:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é“¾æ¥æ¨¡å¼
                        punishment_links = self.driver.find_elements(
                            By.XPATH, 
                            '//a[contains(@href, "ItemDetail")]'
                        )
                    
                    page_punishment_list = []
                    should_stop_pagination = False  # æ ‡å¿—æ˜¯å¦åº”è¯¥åœæ­¢ç¿»é¡µ
                    
                    if use_smart_check:
                        # å¯ç”¨æ™ºèƒ½æ—¥æœŸè¿‡æ»¤ï¼ˆåˆ©ç”¨å€’åºç‰¹æ€§ä¼˜åŒ–ï¼‰
                        for i, link in enumerate(punishment_links):
                            try:
                                href = link.get_attribute('href')
                                title = clean_text(link.text)
                                
                                if href and title:
                                    # è·å–è¯¥é“¾æ¥å¯¹åº”çš„å‘å¸ƒæ—¶é—´
                                    link_publish_date = self.get_link_publish_date(link)
                                    
                                    self.logger.debug(f"æ£€æŸ¥ç¬¬{i+1}æ¡è®°å½•: {title[:50]}... -> æ—¥æœŸ: {link_publish_date}")
                                    
                                    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨ç›®æ ‡æœˆä»½èŒƒå›´å†…
                                    if link_publish_date:
                                        try:
                                            link_date = datetime.strptime(link_publish_date, '%Y-%m-%d')
                                            
                                            if target_start <= link_date < target_end:
                                                # åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œæ·»åŠ åˆ°ç»“æœ
                                                href_full = href if href.startswith('http') else urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                                punishment_info = {
                                                    'title': title,
                                                    'detail_url': href_full,
                                                    'category': category,
                                                    'page': current_page,
                                                    'publish_date': link_publish_date
                                                }
                                                page_punishment_list.append(punishment_info)
                                                found_target_month = True
                                                self.logger.debug(f"âœ“ åœ¨ç›®æ ‡èŒƒå›´å†…: {link_publish_date}")
                                            elif link_date < target_start:
                                                # æ—©äºç›®æ ‡æœˆä»½ï¼Œç”±äºé¡µé¢æ˜¯å€’åºçš„ï¼Œåç»­è®°å½•éƒ½ä¼šæ›´æ—©
                                                self.logger.info(f"âœ— é‡åˆ°æ—©äºç›®æ ‡æœˆä»½çš„è®°å½• ({link_publish_date})ï¼Œåç»­éƒ½ä¼šæ›´æ—©ï¼Œåœæ­¢å¤„ç†")
                                                should_stop_pagination = True  # è®¾ç½®åœæ­¢ç¿»é¡µæ ‡å¿—
                                                break
                                            else:
                                                # æ™šäºç›®æ ‡æœˆä»½ï¼Œç»§ç»­æ£€æŸ¥
                                                self.logger.debug(f"â—‹ æ™šäºç›®æ ‡æœˆä»½ï¼Œç»§ç»­æ£€æŸ¥: {link_publish_date}")
                                        except ValueError as e:
                                            self.logger.debug(f"æ—¥æœŸè§£æå¤±è´¥: {link_publish_date}, {e}")
                                    else:
                                        self.logger.debug("æ— æ³•æå–æ—¥æœŸï¼Œè·³è¿‡è¯¥è®°å½•")
                                        
                            except Exception as e:
                                self.logger.warning(f"å¤„ç†ç¬¬{i+1}æ¡é“¾æ¥å¤±è´¥: {e}")
                                continue
                        
                        total_links = len(punishment_links)
                        filtered_count = len(page_punishment_list)
                        self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {filtered_count} æ¡ç›®æ ‡æœˆä»½è®°å½• (å…±{total_links}æ¡)")
                        
                        # å¦‚æœé‡åˆ°äº†æ—©äºç›®æ ‡æœˆä»½çš„è®°å½•ï¼Œåœæ­¢ç¿»é¡µ
                        if should_stop_pagination:
                            self.logger.info(f"ç¬¬ {current_page} é¡µå·²é‡åˆ°è¶…å‡ºæ—¶é—´æœŸé™çš„å†…å®¹ï¼Œæ— éœ€ç»§ç»­ç¿»é¡µ")
                            all_punishment_list.extend(page_punishment_list)
                            break
                    else:
                        # åŸæœ‰é€»è¾‘ï¼šå¤„ç†æ‰€æœ‰é“¾æ¥
                        for link in punishment_links:
                            try:
                                href = link.get_attribute('href')
                                title = clean_text(link.text)
                                
                                if href and title:
                                    # æ„å»ºå®Œæ•´URL
                                    if not href.startswith('http'):
                                        href = urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                    
                                    punishment_info = {
                                        'title': title,
                                        'detail_url': href,
                                        'category': category,
                                        'page': current_page
                                    }
                                    page_punishment_list.append(punishment_info)
                                    
                            except Exception as e:
                                self.logger.warning(f"è§£æé“¾æ¥å¤±è´¥: {e}")
                                continue
                        
                        self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(page_punishment_list)} æ¡å¤„ç½šä¿¡æ¯")
                    
                    all_punishment_list.extend(page_punishment_list)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µï¼ˆåªæœ‰åœ¨æ²¡æœ‰è®¾ç½®åœæ­¢æ ‡å¿—æ—¶æ‰ç»§ç»­ç¿»é¡µï¼‰
                    if not should_stop_pagination and current_page < max_pages:
                        try:
                            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                            next_buttons = self.driver.find_elements(
                                By.XPATH, 
                                '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                            )
                            
                            if next_buttons:
                                next_button = next_buttons[0]
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                                if next_button.is_enabled():
                                    self.driver.execute_script("arguments[0].click();", next_button)
                                    current_page += 1
                                    time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                else:
                                    self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                    break
                            else:
                                self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                                
                        except Exception as e:
                            self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                            break
                    else:
                        break
                        
                except Exception as e:
                    self.logger.error(f"è§£æç¬¬ {current_page} é¡µå¤±è´¥: {e}")
                    break
            
            # æ™ºèƒ½æ£€æŸ¥ç»“æœåé¦ˆ
            if use_smart_check and not found_target_month:
                self.logger.info(f"{category} æ™ºèƒ½æ£€æŸ¥å®Œæˆï¼Œæœªæ‰¾åˆ° {target_year}å¹´{target_month}æœˆ çš„æ•°æ®")
            
            self.logger.info(f"{category} å¤„ç½šåˆ—è¡¨è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_punishment_list)} æ¡è®°å½•")
            return all_punishment_list
            
        except Exception as e:
            self.logger.error(f"è§£æ {category} å¤„ç½šåˆ—è¡¨å¤±è´¥: {e}")
            return all_punishment_list
    
    def get_punishment_list(self, category: str, max_pages: int = 10) -> List[Dict]:
        """è·å–å¤„ç½šä¿¡æ¯åˆ—è¡¨"""
        url = BASE_URLS.get(category)
        if not url:
            self.logger.error(f"æœªæ‰¾åˆ°ç±»åˆ« '{category}' å¯¹åº”çš„URL")
            return []
        
        if not self.load_page_with_retry(url):
            self.logger.error(f"æ— æ³•åŠ è½½ {category} é¡µé¢")
            return []
        
        all_punishment_list = []
        current_page = 1
        use_smart_check = False  # è¿™ä¸ªæ–¹æ³•ä¸æ”¯æŒæ™ºèƒ½æ£€æŸ¥
        
        try:
            while current_page <= max_pages:
                self.logger.info(f"æ­£åœ¨è§£æ {category} ç¬¬ {current_page} é¡µ")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œä¼˜åŒ–ç­‰å¾…æ—¶é—´
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(0.5)  # å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ä»2ç§’åˆ°0.5ç§’
                
                # æŸ¥æ‰¾åŒ…å«"è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€è¡¨"çš„é“¾æ¥
                try:
                    # ä½¿ç”¨æ›´å®½æ³›çš„XPathé€‰æ‹©å™¨
                    punishment_links = self.driver.find_elements(
                        By.XPATH, 
                        '//a[contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬ç¤º") or contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€") or contains(text(), "å¤„ç½šä¿¡æ¯")]'
                    )
                    
                    if not punishment_links:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é“¾æ¥æ¨¡å¼
                        punishment_links = self.driver.find_elements(
                            By.XPATH, 
                            '//a[contains(@href, "ItemDetail")]'
                        )
                    
                    page_punishment_list = []
                    
                    if use_smart_check:
                        # å¯¹æ¯ä¸ªé“¾æ¥æ£€æŸ¥å…¶å¯¹åº”çš„å‘å¸ƒæ—¥æœŸï¼ˆåˆ©ç”¨å€’åºç‰¹æ€§ä¼˜åŒ–ï¼‰
                        target_date_found = False
                        for i, link in enumerate(punishment_links):
                            try:
                                href = link.get_attribute('href')
                                title = clean_text(link.text)
                                
                                if href and title:
                                    # è·å–è¯¥é“¾æ¥å¯¹åº”çš„å‘å¸ƒæ—¶é—´
                                    link_publish_date = self.get_link_publish_date(link)
                                    
                                    self.logger.debug(f"æ£€æŸ¥ç¬¬{i+1}æ¡è®°å½•: {title[:50]}... -> æ—¥æœŸ: {link_publish_date}")
                                    
                                    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨ç›®æ ‡æœˆä»½èŒƒå›´å†…
                                    if link_publish_date:
                                        try:
                                            link_date = datetime.strptime(link_publish_date, '%Y-%m-%d')
                                            
                                            if target_start <= link_date < target_end:
                                                # åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œæ·»åŠ åˆ°ç»“æœ
                                                href_full = href if href.startswith('http') else urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                                punishment_info = {
                                                    'title': title,
                                                    'detail_url': href_full,
                                                    'category': category,
                                                    'page': current_page,
                                                    'publish_date': link_publish_date
                                                }
                                                page_punishment_list.append(punishment_info)
                                                target_date_found = True
                                                self.logger.debug(f"âœ“ åœ¨ç›®æ ‡èŒƒå›´å†…: {link_publish_date}")
                                            elif link_date < target_start:
                                                # æ—©äºç›®æ ‡æœˆä»½ï¼Œç”±äºé¡µé¢æ˜¯å€’åºçš„ï¼Œåç»­è®°å½•éƒ½ä¼šæ›´æ—©ï¼Œå¯ä»¥åœæ­¢æ£€æŸ¥
                                                self.logger.debug(f"âœ— æ—©äºç›®æ ‡æœˆä»½ï¼Œåç»­éƒ½ä¼šæ›´æ—©ï¼Œåœæ­¢æ£€æŸ¥: {link_publish_date}")
                                                break
                                            else:
                                                # æ™šäºç›®æ ‡æœˆä»½ï¼Œç»§ç»­æ£€æŸ¥
                                                self.logger.debug(f"â—‹ æ™šäºç›®æ ‡æœˆä»½ï¼Œç»§ç»­æ£€æŸ¥: {link_publish_date}")
                                        except ValueError as e:
                                            self.logger.debug(f"æ—¥æœŸè§£æå¤±è´¥: {link_publish_date}, {e}")
                                    else:
                                        self.logger.debug("æ— æ³•æå–æ—¥æœŸï¼Œè·³è¿‡è¯¥è®°å½•")
                                        
                            except Exception as e:
                                self.logger.warning(f"å¤„ç†ç¬¬{i+1}æ¡é“¾æ¥å¤±è´¥: {e}")
                                continue
                    else:
                        # åŸæœ‰é€»è¾‘ï¼šå¤„ç†æ‰€æœ‰é“¾æ¥
                        for link in punishment_links:
                            try:
                                href = link.get_attribute('href')
                                title = clean_text(link.text)
                                
                                if href and title:
                                    # æ„å»ºå®Œæ•´URL
                                    if not href.startswith('http'):
                                        href = urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                    
                                    punishment_info = {
                                        'title': title,
                                        'detail_url': href,
                                        'category': category,
                                        'page': current_page
                                    }
                                    page_punishment_list.append(punishment_info)
                                    
                            except Exception as e:
                                self.logger.warning(f"è§£æé“¾æ¥å¤±è´¥: {e}")
                                continue
                    
                    if use_smart_check:
                        total_links = len(punishment_links)
                        filtered_count = len(page_punishment_list)
                        self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {filtered_count} æ¡ç›®æ ‡æœˆä»½è®°å½• (å…±{total_links}æ¡)")
                    else:
                        self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(page_punishment_list)} æ¡å¤„ç½šä¿¡æ¯")
                    all_punishment_list.extend(page_punishment_list)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    if current_page < max_pages:
                        try:
                            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                            next_buttons = self.driver.find_elements(
                                By.XPATH, 
                                '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                            )
                            
                            if next_buttons:
                                next_button = next_buttons[0]
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                                if next_button.is_enabled():
                                    self.driver.execute_script("arguments[0].click();", next_button)
                                    current_page += 1
                                    time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                else:
                                    self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                    break
                            else:
                                self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                                
                        except Exception as e:
                            self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                            break
                    else:
                        break
                        
                except Exception as e:
                    self.logger.error(f"è§£æç¬¬ {current_page} é¡µå¤±è´¥: {e}")
                    break
            
            self.logger.info(f"{category} å¤„ç½šåˆ—è¡¨è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_punishment_list)} æ¡è®°å½•")
            return all_punishment_list
            
        except Exception as e:
            self.logger.error(f"è§£æ {category} å¤„ç½šåˆ—è¡¨å¤±è´¥: {e}")
            return all_punishment_list
    
    def get_punishment_detail(self, detail_url: str) -> Dict:
        """è·å–å¤„ç½šè¯¦æƒ…"""
        if not self.load_page_with_retry(detail_url):
            self.logger.error(f"æ— æ³•åŠ è½½è¯¦æƒ…é¡µé¢: {detail_url}")
            return {}
        
        try:
            # ç­‰å¾…è¡¨æ ¼åŠ è½½ - æ”¯æŒå¤šç§è¡¨æ ¼ç±»å‹
            table = None
            try:
                # é¦–å…ˆå°è¯•æŸ¥æ‰¾ MsoTableGrid ç±»å‹çš„è¡¨æ ¼
                table = self.wait.until(
                    EC.presence_of_element_located((By.CLASS_NAME, "MsoTableGrid"))
                )
            except:
                try:
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ MsoNormalTable ç±»å‹çš„è¡¨æ ¼
                    table = self.wait.until(
                        EC.presence_of_element_located((By.CLASS_NAME, "MsoNormalTable"))
                    )
                except:
                    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ä»»ä½•tableå…ƒç´ 
                    tables = self.driver.find_elements(By.TAG_NAME, "table")
                    if tables:
                        table = tables[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¡¨æ ¼
            
            if table:
                # è§£æè¡¨æ ¼æ•°æ®
                detail_data = self.parse_punishment_table(table)
                
                # æ·»åŠ æŠ“å–æ—¶é—´å’Œé“¾æ¥
                detail_data['æŠ“å–æ—¶é—´'] = get_current_timestamp()
                detail_data['è¯¦æƒ…é“¾æ¥'] = detail_url
                
                return detail_data
            else:
                self.logger.error(f"æœªæ‰¾åˆ°è¡¨æ ¼å…ƒç´ : {detail_url}")
                return {}
                
        except TimeoutException:
            self.logger.error(f"è¯¦æƒ…é¡µé¢è¡¨æ ¼åŠ è½½è¶…æ—¶: {detail_url}")
        except Exception as e:
            self.logger.error(f"è§£æè¯¦æƒ…é¡µé¢å¤±è´¥: {e}")
        
        return {}
    
    def parse_punishment_table(self, table_element) -> Dict:
        """è§£æå¤„ç½šä¿¡æ¯è¡¨æ ¼ - å¢å¼ºç‰ˆæœ¬æ”¯æŒå¤šç§è¡¨æ ¼æ ¼å¼"""
        try:
            # å°†Seleniumå…ƒç´ è½¬æ¢ä¸ºBeautifulSoupå¯¹è±¡è¿›è¡Œè§£æ
            table_html = table_element.get_attribute('outerHTML')
            soup = BeautifulSoup(table_html, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return {}
            
            # é¦–å…ˆå°è¯•æå–å®Œæ•´çš„é¡µé¢æ–‡æœ¬å†…å®¹ç”¨äºå¤‡ç”¨è§£æ
            page_text = clean_text(table.get_text())
            self.logger.debug(f"é¡µé¢æ–‡æœ¬å†…å®¹é•¿åº¦: {len(page_text)}")
            
            # å°è¯•ä»å®Œæ•´æ–‡æœ¬ä¸­æå–è¡Œæ”¿å¤„ç½šä¾æ®
            punishment_basis = self.extract_punishment_basis_from_text(page_text)
            
            # åˆå§‹åŒ–æ•°æ®å­—å…¸
            data = {}
            if punishment_basis:
                data['è¡Œæ”¿å¤„ç½šä¾æ®'] = punishment_basis
            
            rows = table.find_all('tr')
            
            if len(rows) < 2:
                return data if data else {}
            
            # æ£€æŸ¥è¡¨æ ¼ç±»å‹ï¼šæ¨ªå‘å¤šåˆ— vs é”®å€¼å¯¹
            first_row = rows[0]
            first_row_cells = first_row.find_all(['td', 'th'])
            
            # å¦‚æœç¬¬ä¸€è¡Œæœ‰å¤šä¸ªåˆ—ï¼ˆ>=3ï¼‰ï¼Œä¸”åŒ…å«å¸¸è§çš„è¡¨å¤´å…³é”®è¯ï¼Œåˆ™ä¸ºæ¨ªå‘è¡¨æ ¼
            if len(first_row_cells) >= 3:
                header_texts = [clean_text(cell.get_text()) for cell in first_row_cells]
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„è¡¨å¤´å…³é”®è¯
                header_keywords = ['åºå·', 'å½“äº‹äºº', 'è¿æ³•', 'å¤„ç½š', 'æœºå…³']
                if any(any(keyword in header for keyword in header_keywords) for header in header_texts):
                    self.logger.info("æ£€æµ‹åˆ°æ¨ªå‘å¤šåˆ—è¡¨æ ¼ï¼Œä½¿ç”¨æ¨ªå‘è§£æé€»è¾‘")
                    table_data = self.parse_horizontal_table(rows)
                    data.update(table_data)
                    return data
            
            # å¦åˆ™ä½¿ç”¨é”®å€¼å¯¹è§£æé€»è¾‘
            self.logger.info("æ£€æµ‹åˆ°é”®å€¼å¯¹è¡¨æ ¼ï¼Œä½¿ç”¨é”®å€¼å¯¹è§£æé€»è¾‘")
            table_data = self.parse_key_value_table(rows)
            data.update(table_data)
            
            # å¦‚æœé”®å€¼å¯¹è§£ææ²¡æœ‰æ‰¾åˆ°å¤„ç½šå†…å®¹ï¼Œå°è¯•ä»å®Œæ•´æ–‡æœ¬ä¸­æå–
            if 'è¡Œæ”¿å¤„ç½šå†…å®¹' not in data or not data['è¡Œæ”¿å¤„ç½šå†…å®¹']:
                punishment_content = self.extract_punishment_content_from_text(page_text)
                if punishment_content:
                    data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = punishment_content
            
            return data
            
        except Exception as e:
            self.logger.error(f"è§£æå¤„ç½šè¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def extract_punishment_basis_from_text(self, text: str) -> str:
        """ä»å®Œæ•´æ–‡æœ¬ä¸­æå–è¡Œæ”¿å¤„ç½šä¾æ®"""
        try:
            import re
            
            # å¸¸è§çš„å¤„ç½šä¾æ®å…³é”®è¯æ¨¡å¼
            basis_patterns = [
                # å®Œæ•´çš„æ³•æ¡å¼•ç”¨
                r'ä¾æ®[ã€Š]?([^ã€‹ã€‚ï¼›\n]+ç¬¬[^ã€‚ï¼›\n]+æ¡[^ã€‚ï¼›\n]*)[ã€‹]?',
                r'æ ¹æ®[ã€Š]?([^ã€‹ã€‚ï¼›\n]+ç¬¬[^ã€‚ï¼›\n]+æ¡[^ã€‚ï¼›\n]*)[ã€‹]?',
                r'æŒ‰ç…§[ã€Š]?([^ã€‹ã€‚ï¼›\n]+ç¬¬[^ã€‚ï¼›\n]+æ¡[^ã€‚ï¼›\n]*)[ã€‹]?',
                # æ³•å¾‹æ³•è§„åç§°
                r'ã€Š([^ã€‹]+æ³•[^ã€‹]*)ã€‹',
                r'ã€Š([^ã€‹]+è§„å®š[^ã€‹]*)ã€‹',
                r'ã€Š([^ã€‹]+åŠæ³•[^ã€‹]*)ã€‹',
                r'ã€Š([^ã€‹]+æ¡ä¾‹[^ã€‹]*)ã€‹',
                # ç®€å•çš„ä¾æ®å¼•ç”¨
                r'ä¾æ®[ã€Š]?([^ã€‹ã€‚ï¼›\n]+)[ã€‹]?[ï¼Œã€‚]',
                r'æ ¹æ®[ã€Š]?([^ã€‹ã€‚ï¼›\n]+)[ã€‹]?[ï¼Œã€‚]',
                r'è¿å[äº†]?[ã€Š]?([^ã€‹ã€‚ï¼›\n]+)[ã€‹]?',
            ]
            
            found_basis = []
            
            for pattern in basis_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if match and len(match.strip()) > 3:
                        clean_match = clean_text(match).strip()
                        # é¿å…é‡å¤å’Œè¿‡çŸ­çš„åŒ¹é…
                        if clean_match not in found_basis and len(clean_match) > 5:
                            found_basis.append(clean_match)
            
            if found_basis:
                basis_text = "ï¼›".join(found_basis)
                self.logger.debug(f"æå–åˆ°è¡Œæ”¿å¤„ç½šä¾æ®: {basis_text}")
                return basis_text
            
            return ""
            
        except Exception as e:
            self.logger.warning(f"æå–è¡Œæ”¿å¤„ç½šä¾æ®å¤±è´¥: {e}")
            return ""
    
    def extract_punishment_content_from_text(self, text: str) -> str:
        """ä»å®Œæ•´æ–‡æœ¬ä¸­æå–å®Œæ•´çš„è¡Œæ”¿å¤„ç½šå†…å®¹"""
        try:
            import re
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤„ç½šç›¸å…³çš„å¥å­
            punishment_sentences = []
            
            # æ”¹è¿›çš„åŒ¹é…"å¯¹...å¤„ç½š"æ¨¡å¼
            fine_patterns = [
                # åŒ¹é…ï¼šå¯¹XXXè­¦å‘Šå¹¶ç½šæ¬¾Xä¸‡å…ƒ
                r'å¯¹([^å¯¹ã€‚ï¼›\n]+?)(è­¦å‘Šå¹¶ç½šæ¬¾[0-9]+ä¸‡å…ƒ)',
                # åŒ¹é…ï¼šå¯¹XXXç½šæ¬¾Xä¸‡å…ƒ
                r'å¯¹([^å¯¹ã€‚ï¼›\n]+?)(ç½šæ¬¾[0-9]+ä¸‡å…ƒ[^ã€‚ï¼›\n]*)',
                # åŒ¹é…ï¼šå¯¹XXXè­¦å‘Š
                r'å¯¹([^å¯¹ã€‚ï¼›\n]+?)(è­¦å‘Š[^ã€‚ï¼›\n]*)',
            ]
            
            for pattern in fine_patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
                for match in matches:
                    if isinstance(match, tuple) and len(match) == 2:
                        # å¯¹å½“äº‹äººçš„å¤„ç½š
                        person = clean_text(match[0]).strip()
                        punishment = clean_text(match[1]).strip()
                        if person and punishment:
                            sentence = f"å¯¹{person}{punishment}"
                            punishment_sentences.append(sentence)
            
            # å•ç‹¬æå–åˆè®¡ä¿¡æ¯å’Œè¯¦ç»†è¯´æ˜
            summary_patterns = [
                r'(åˆè®¡ç½šæ¬¾[0-9]+ä¸‡å…ƒ[^ã€‚ï¼›\n]*)',
                r'(å…¶ä¸­[^ã€‚ï¼›\n]*[0-9]+ä¸‡å…ƒ[^ã€‚ï¼›\n]*)',
            ]
            
            for pattern in summary_patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
                for match in matches:
                    sentence = clean_text(match).strip()
                    if sentence and len(sentence) > 3:
                        punishment_sentences.append(sentence)
            
            # ä½¿ç”¨è¡Œåˆ†å‰²çš„æ–¹å¼é‡æ–°æå–ï¼Œç¡®ä¿ä¸é—æ¼
            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æŸ¥æ‰¾åŒ…å«å¤„ç½šå…³é”®è¯çš„è¡Œ
                if any(keyword in line for keyword in ['å¯¹', 'ç½šæ¬¾', 'ä¸‡å…ƒ', 'è­¦å‘Š']):
                    # è¿›ä¸€æ­¥å¤„ç†æ¯ä¸€è¡Œ
                    # åˆ†å‰²å¯èƒ½åŒ…å«å¤šä¸ªå¤„ç½šçš„è¡Œ
                    sub_sentences = re.split(r'(?=å¯¹[^å¯¹]*(?:è­¦å‘Š|ç½šæ¬¾))', line)
                    for sub_sentence in sub_sentences:
                        sub_sentence = sub_sentence.strip()
                        if len(sub_sentence) > 3 and ('ç½šæ¬¾' in sub_sentence or 'è­¦å‘Š' in sub_sentence):
                            clean_sentence = clean_text(sub_sentence)
                            # é¿å…é‡å¤æ·»åŠ 
                            if clean_sentence not in punishment_sentences:
                                punishment_sentences.append(clean_sentence)
            
            # æ™ºèƒ½å»é‡ï¼šç§»é™¤å®Œå…¨åŒ…å«åœ¨å…¶ä»–å¥å­ä¸­çš„å†…å®¹
            unique_sentences = []
            for sentence in punishment_sentences:
                if sentence and len(sentence) > 3:
                    sentence = sentence.strip('ã€‚ï¼›ï¼Œ')
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«å…¶ä»–å¥å­åŒ…å«
                    is_duplicate = False
                    for existing in unique_sentences:
                        if sentence in existing or existing in sentence:
                            # ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬
                            if len(sentence) > len(existing):
                                unique_sentences.remove(existing)
                                break
                            else:
                                is_duplicate = True
                                break
                    
                    if not is_duplicate:
                        unique_sentences.append(sentence)
            
            if unique_sentences:
                content = "ï¼›".join(unique_sentences)
                self.logger.debug(f"æå–åˆ°è¡Œæ”¿å¤„ç½šå†…å®¹ï¼Œå…±{len(unique_sentences)}æ¡: {content[:200]}...")
                return content
            
            return ""
            
        except Exception as e:
            self.logger.warning(f"æå–è¡Œæ”¿å¤„ç½šå†…å®¹å¤±è´¥: {e}")
            return ""
    
    def parse_horizontal_table(self, rows: list) -> Dict:
        """è§£ææ¨ªå‘å¤šåˆ—è¡¨æ ¼ï¼Œæ”¯æŒå¤šè¡Œæ•°æ®"""
        try:
            if len(rows) < 2:
                return {}
            
            # è§£æè¡¨å¤´
            header_row = rows[0]
            header_cells = header_row.find_all(['td', 'th'])
            headers = [clean_text(cell.get_text()) for cell in header_cells]
            
            self.logger.debug(f"è¡¨å¤´: {headers}")
            
            # å¦‚æœåªæœ‰2è¡Œï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆå•æ¡è®°å½•ï¼‰
            if len(rows) == 2:
                return self.parse_single_row_table(headers, rows[1])
            
            # å¤šè¡Œæ•°æ®å¤„ç†
            elif len(rows) > 2:
                return self.parse_multi_row_table(headers, rows[1:])
            
            return {}
            
        except Exception as e:
            self.logger.error(f"è§£ææ¨ªå‘è¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def parse_single_row_table(self, headers: list, data_row) -> Dict:
        """è§£æå•è¡Œæ•°æ®è¡¨æ ¼"""
        try:
            data_cells = data_row.find_all(['td', 'th'])
            
            if len(data_cells) < len(headers):
                self.logger.warning(f"æ•°æ®åˆ—æ•°({len(data_cells)})å°‘äºè¡¨å¤´åˆ—æ•°({len(headers)})")
                return {}
            
            data = {}
            
            # æ ¹æ®è¡¨å¤´æ˜ å°„æ•°æ®
            for i, header in enumerate(headers):
                if i < len(data_cells):
                    cell_text = clean_text(data_cells[i].get_text())
                    
                    # å­—æ®µæ˜ å°„
                    if any(keyword in header for keyword in ['åºå·']):
                        data['åºå·'] = cell_text
                    elif any(keyword in header for keyword in ['å½“äº‹äºº', 'è¢«å¤„ç½šå½“äº‹äºº']):
                        data['å½“äº‹äººåç§°'] = cell_text
                    elif any(keyword in header for keyword in ['è¿æ³•è¿è§„', 'ä¸»è¦è¿æ³•è¿è§„', 'è¿æ³•è¡Œä¸º', 'è¿æ³•è¿è§„äº‹å®', 'ä¸»è¦è¿æ³•è¿è§„äº‹å®']):
                        data['ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º'] = cell_text
                    elif any(keyword in header for keyword in ['å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½š', 'å¤„ç½šå†³å®š', 'è¡Œæ”¿å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½šå†³å®š']):
                        data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šæœºå…³', 'æœºå…³åç§°', 'ä½œå‡ºå†³å®šæœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°']):
                        data['ä½œå‡ºå†³å®šæœºå…³'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šæ—¥æœŸ', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ', 'å¤„ç½šå†³å®šæ—¥æœŸ']):
                        data['ä½œå‡ºå†³å®šæ—¥æœŸ'] = cell_text
                    elif any(keyword in header for keyword in ['å¤„ç½šä¾æ®', 'è¡Œæ”¿å¤„ç½šä¾æ®']):
                        data['è¡Œæ”¿å¤„ç½šä¾æ®'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šä¹¦æ–‡å·', 'è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·']):
                        data['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·'] = cell_text
                    else:
                        # ä¿å­˜å…¶ä»–å­—æ®µ
                        data[header] = cell_text
            
            self.logger.debug(f"å•è¡Œè¡¨æ ¼è§£æç»“æœ: {data}")
            return data
            
        except Exception as e:
            self.logger.error(f"è§£æå•è¡Œè¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def parse_multi_row_table(self, headers: list, data_rows: list) -> Dict:
        """è§£æå¤šè¡Œæ•°æ®è¡¨æ ¼ï¼Œè¿”å›å¤šæ¡è®°å½•"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰åˆå¹¶å•å…ƒæ ¼
            if self.has_merged_cells(data_rows):
                self.logger.info("æ£€æµ‹åˆ°åˆå¹¶å•å…ƒæ ¼ï¼Œä½¿ç”¨åˆå¹¶å•å…ƒæ ¼è§£æé€»è¾‘")
                return self.parse_merged_cells_table(headers, data_rows)
            
            # åŸæœ‰çš„å¤šè¡Œè¡¨æ ¼è§£æé€»è¾‘
            all_records = []
            
            for row_index, data_row in enumerate(data_rows, 1):
                data_cells = data_row.find_all(['td', 'th'])
                
                if len(data_cells) < len(headers):
                    self.logger.warning(f"ç¬¬{row_index}è¡Œæ•°æ®åˆ—æ•°({len(data_cells)})å°‘äºè¡¨å¤´åˆ—æ•°({len(headers)})")
                    continue
                
                record = {}
                
                # æ ¹æ®è¡¨å¤´æ˜ å°„æ•°æ®
                for i, header in enumerate(headers):
                    if i < len(data_cells):
                        cell_text = clean_text(data_cells[i].get_text())
                        
                        # å­—æ®µæ˜ å°„
                        if any(keyword in header for keyword in ['åºå·']):
                            record['åºå·'] = cell_text
                            record['åŸå§‹åºå·'] = cell_text  # ä¿ç•™åŸå§‹åºå·
                        elif any(keyword in header for keyword in ['å½“äº‹äºº', 'è¢«å¤„ç½šå½“äº‹äºº']):
                            record['å½“äº‹äººåç§°'] = cell_text
                        elif any(keyword in header for keyword in ['è¿æ³•è¿è§„', 'ä¸»è¦è¿æ³•è¿è§„', 'è¿æ³•è¡Œä¸º', 'è¿æ³•è¿è§„äº‹å®', 'ä¸»è¦è¿æ³•è¿è§„äº‹å®']):
                            record['ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º'] = cell_text
                        elif any(keyword in header for keyword in ['å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½š', 'å¤„ç½šå†³å®š', 'è¡Œæ”¿å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½šå†³å®š']):
                            record['è¡Œæ”¿å¤„ç½šå†…å®¹'] = cell_text
                        elif any(keyword in header for keyword in ['å†³å®šæœºå…³', 'æœºå…³åç§°', 'ä½œå‡ºå†³å®šæœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°']):
                            record['ä½œå‡ºå†³å®šæœºå…³'] = cell_text
                        elif any(keyword in header for keyword in ['å†³å®šæ—¥æœŸ', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ', 'å¤„ç½šå†³å®šæ—¥æœŸ']):
                            record['ä½œå‡ºå†³å®šæ—¥æœŸ'] = cell_text
                        elif any(keyword in header for keyword in ['å¤„ç½šä¾æ®', 'è¡Œæ”¿å¤„ç½šä¾æ®']):
                            record['è¡Œæ”¿å¤„ç½šä¾æ®'] = cell_text
                        elif any(keyword in header for keyword in ['å†³å®šä¹¦æ–‡å·', 'è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·']):
                            record['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·'] = cell_text
                        else:
                            # ä¿å­˜å…¶ä»–å­—æ®µ
                            record[header] = cell_text
                
                # æ·»åŠ è®°å½•æ ‡è¯†ä¿¡æ¯
                record['æ‰¹æ–‡å†…åºå·'] = row_index
                record['æ˜¯å¦å¤šè®°å½•æ‰¹æ–‡'] = 'æ˜¯'
                
                if any(record.values()):  # åªæ·»åŠ éç©ºè®°å½•
                    all_records.append(record)
                    self.logger.debug(f"è§£æç¬¬{row_index}æ¡è®°å½•: {record.get('å½“äº‹äººåç§°', 'æœªçŸ¥')}")
            
            # è¿”å›æ ¼å¼ï¼šå¦‚æœæœ‰å¤šæ¡è®°å½•ï¼Œä½¿ç”¨ç‰¹æ®Šæ ¼å¼
            if len(all_records) > 1:
                self.logger.info(f"æ£€æµ‹åˆ°å¤šè®°å½•æ‰¹æ–‡ï¼Œå…±{len(all_records)}æ¡è®°å½•")
                # è¿”å›ç¬¬ä¸€æ¡è®°å½•ä½œä¸ºä¸»è®°å½•ï¼Œå…¶ä»–è®°å½•ä½œä¸ºadditional_records
                result = all_records[0].copy()
                result['additional_records'] = all_records[1:]
                result['è®°å½•æ€»æ•°'] = len(all_records)
                return result
            elif len(all_records) == 1:
                all_records[0]['æ˜¯å¦å¤šè®°å½•æ‰¹æ–‡'] = 'å¦'
                return all_records[0]
            else:
                self.logger.warning("å¤šè¡Œè¡¨æ ¼æœªè§£æåˆ°æœ‰æ•ˆæ•°æ®")
                return {}
            
        except Exception as e:
            self.logger.error(f"è§£æå¤šè¡Œè¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def has_merged_cells(self, data_rows: list) -> bool:
        """æ£€æµ‹è¡¨æ ¼æ˜¯å¦æœ‰åˆå¹¶å•å…ƒæ ¼"""
        try:
            for row in data_rows:
                cells = row.find_all(['td', 'th'])
                for cell in cells:
                    rowspan = cell.get('rowspan')
                    colspan = cell.get('colspan') 
                    if (rowspan and int(rowspan) > 1) or (colspan and int(colspan) > 1):
                        return True
            return False
        except Exception as e:
            self.logger.warning(f"æ£€æµ‹åˆå¹¶å•å…ƒæ ¼å¤±è´¥: {e}")
            return False
    
    def parse_merged_cells_table(self, headers: list, data_rows: list) -> Dict:
        """è§£æåŒ…å«åˆå¹¶å•å…ƒæ ¼çš„è¡¨æ ¼"""
        try:
            all_records = []
            
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç½‘æ ¼æ¥è¿½è¸ªåˆå¹¶å•å…ƒæ ¼
            grid = {}
            max_cols = len(headers)
            
            for row_idx, row in enumerate(data_rows):
                cells = row.find_all(['td', 'th'])
                cell_idx = 0
                col_idx = 0
                
                for cell in cells:
                    # è·³è¿‡è¢«åˆå¹¶å•å…ƒæ ¼å ç”¨çš„ä½ç½®
                    while (row_idx, col_idx) in grid:
                        col_idx += 1
                    
                    # è·å–å•å…ƒæ ¼å†…å®¹
                    cell_text = clean_text(cell.get_text())
                    
                    # è·å–åˆå¹¶å±æ€§
                    rowspan = int(cell.get('rowspan', 1))
                    colspan = int(cell.get('colspan', 1))
                    
                    # åœ¨ç½‘æ ¼ä¸­æ ‡è®°è¿™ä¸ªå•å…ƒæ ¼åŠå…¶åˆå¹¶èŒƒå›´
                    for r in range(row_idx, row_idx + rowspan):
                        for c in range(col_idx, col_idx + colspan):
                            if c < max_cols:  # ç¡®ä¿ä¸è¶…å‡ºè¡¨å¤´èŒƒå›´
                                grid[(r, c)] = {
                                    'text': cell_text,
                                    'original_row': row_idx,
                                    'original_col': col_idx,
                                    'rowspan': rowspan,
                                    'colspan': colspan
                                }
                    
                    col_idx += colspan
                    cell_idx += 1
            
            # ç°åœ¨æ ¹æ®ç½‘æ ¼æ•°æ®æ„å»ºè®°å½•
            processed_rows = {}
            
            for (row_idx, col_idx), cell_info in grid.items():
                if row_idx not in processed_rows:
                    processed_rows[row_idx] = {}
                
                if col_idx < len(headers):
                    header = headers[col_idx]
                    processed_rows[row_idx][header] = cell_info['text']
            
            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            for row_idx in sorted(processed_rows.keys()):
                row_data = processed_rows[row_idx]
                
                # å¯¹äºåˆå¹¶å•å…ƒæ ¼ï¼Œéœ€è¦ä»å‰é¢çš„è¡Œç»§æ‰¿æ•°æ®
                if row_idx > 0:
                    # æ£€æŸ¥å“ªäº›å­—æ®µåœ¨è¿™ä¸€è¡Œæ˜¯ç©ºçš„ï¼ˆå¯èƒ½è¢«åˆå¹¶å•å…ƒæ ¼è·¨è¡Œè¦†ç›–ï¼‰
                    for col_idx, header in enumerate(headers):
                        if header not in row_data or not row_data[header]:
                            # æŸ¥æ‰¾è¿™ä¸ªä½ç½®æ˜¯å¦è¢«å‰é¢è¡Œçš„åˆå¹¶å•å…ƒæ ¼è¦†ç›–
                            for prev_row_idx in range(row_idx - 1, -1, -1):
                                cell_key = (prev_row_idx, col_idx)
                                if cell_key in grid:
                                    cell_info = grid[cell_key]
                                    # æ£€æŸ¥è¿™ä¸ªåˆå¹¶å•å…ƒæ ¼æ˜¯å¦è¦†ç›–å½“å‰è¡Œ
                                    if (prev_row_idx + cell_info['rowspan'] > row_idx):
                                        row_data[header] = cell_info['text']
                                        break
                
                # æ„å»ºè®°å½•
                record = {}
                
                for header, cell_text in row_data.items():
                    if not cell_text:
                        continue
                        
                    # å­—æ®µæ˜ å°„é€»è¾‘
                    if any(keyword in header for keyword in ['åºå·']):
                        record['åºå·'] = cell_text
                        record['åŸå§‹åºå·'] = cell_text
                    elif any(keyword in header for keyword in ['å½“äº‹äºº', 'è¢«å¤„ç½šå½“äº‹äºº']):
                        record['å½“äº‹äººåç§°'] = cell_text
                    elif any(keyword in header for keyword in ['è¿æ³•è¿è§„', 'ä¸»è¦è¿æ³•è¿è§„', 'è¿æ³•è¡Œä¸º', 'è¿æ³•è¿è§„äº‹å®', 'ä¸»è¦è¿æ³•è¿è§„äº‹å®']):
                        record['ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º'] = cell_text
                    elif any(keyword in header for keyword in ['å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½š', 'å¤„ç½šå†³å®š', 'è¡Œæ”¿å¤„ç½šå†…å®¹', 'è¡Œæ”¿å¤„ç½šå†³å®š']):
                        record['è¡Œæ”¿å¤„ç½šå†…å®¹'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šæœºå…³', 'æœºå…³åç§°', 'ä½œå‡ºå†³å®šæœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°']):
                        record['ä½œå‡ºå†³å®šæœºå…³'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šæ—¥æœŸ', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ', 'å¤„ç½šå†³å®šæ—¥æœŸ']):
                        record['ä½œå‡ºå†³å®šæ—¥æœŸ'] = cell_text
                    elif any(keyword in header for keyword in ['å¤„ç½šä¾æ®', 'è¡Œæ”¿å¤„ç½šä¾æ®']):
                        record['è¡Œæ”¿å¤„ç½šä¾æ®'] = cell_text
                    elif any(keyword in header for keyword in ['å†³å®šä¹¦æ–‡å·', 'è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·']):
                        record['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·'] = cell_text
                    else:
                        record[header] = cell_text
                
                # æ·»åŠ è®°å½•æ ‡è¯†ä¿¡æ¯
                record['æ‰¹æ–‡å†…åºå·'] = row_idx + 1
                record['æ˜¯å¦å¤šè®°å½•æ‰¹æ–‡'] = 'æ˜¯'
                
                # åªæ·»åŠ æœ‰å®é™…å½“äº‹äººæ•°æ®çš„è®°å½•ï¼ˆé¿å…æ·»åŠ åªæœ‰åˆå¹¶å•å…ƒæ ¼æ•°æ®çš„ç©ºè®°å½•ï¼‰
                if record.get('å½“äº‹äººåç§°') and record['å½“äº‹äººåç§°'].strip():
                    all_records.append(record)
                    self.logger.info(f"è§£æåˆå¹¶å•å…ƒæ ¼è®°å½•: {record.get('å½“äº‹äººåç§°', 'æœªçŸ¥')}")
            
            # è¿”å›ç»“æœ
            if len(all_records) > 1:
                self.logger.info(f"åˆå¹¶å•å…ƒæ ¼è¡¨æ ¼è§£æå®Œæˆï¼Œå…±{len(all_records)}æ¡è®°å½•")
                result = all_records[0].copy()
                result['additional_records'] = all_records[1:]
                result['è®°å½•æ€»æ•°'] = len(all_records)
                return result
            elif len(all_records) == 1:
                all_records[0]['æ˜¯å¦å¤šè®°å½•æ‰¹æ–‡'] = 'å¦'
                return all_records[0]
            else:
                self.logger.warning("åˆå¹¶å•å…ƒæ ¼è¡¨æ ¼æœªè§£æåˆ°æœ‰æ•ˆæ•°æ®")
                return {}
                
        except Exception as e:
            self.logger.error(f"è§£æåˆå¹¶å•å…ƒæ ¼è¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def parse_key_value_table(self, rows: list) -> Dict:
        """è§£æé”®å€¼å¯¹è¡¨æ ¼ï¼ˆå·¦å³ä¸¤åˆ—ï¼‰"""
        try:
            data = {}
            
            # éå†æ‰€æœ‰è¡Œï¼ŒæŸ¥æ‰¾å­—æ®µå¯¹åº”å…³ç³»
            for row in rows:
                cells = row.find_all(['td', 'th'])
                
                if len(cells) >= 2:
                    # è·å–å·¦å³ä¸¤åˆ—çš„å†…å®¹
                    left_cell = cells[0]
                    right_cell = cells[1]
                    
                    left_text = clean_text(left_cell.get_text())
                    right_text = clean_text(right_cell.get_text())
                    
                    # è·³è¿‡ç©ºå†…å®¹
                    if not left_text or not right_text:
                        continue
                    
                    # å¢å¼ºçš„å­—æ®µæ˜ å°„è§„åˆ™ - ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„å­—æ®µå
                    # ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
                    if left_text in ['åºå·']:
                        data['åºå·'] = right_text
                    elif left_text in ['å½“äº‹äººåç§°', 'è¢«å¤„ç½šå½“äº‹äºº', 'å½“äº‹äºº']:
                        data['å½“äº‹äººåç§°'] = right_text
                    elif left_text in ['ä¸»è¦è¿æ³•è¿è§„äº‹å®', 'ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º', 'è¿æ³•è¿è§„äº‹å®', 'è¿æ³•è¿è§„è¡Œä¸º', 'è¿æ³•è¡Œä¸º']:
                        data['ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º'] = right_text
                    elif left_text in ['è¡Œæ”¿å¤„ç½šä¾æ®', 'å¤„ç½šä¾æ®']:
                        data['è¡Œæ”¿å¤„ç½šä¾æ®'] = right_text
                    elif left_text in ['è¡Œæ”¿å¤„ç½šå†³å®š', 'å¤„ç½šå†³å®š']:
                        data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = right_text
                    elif left_text in ['è¡Œæ”¿å¤„ç½šå†…å®¹', 'å¤„ç½šå†…å®¹']:
                        data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = right_text
                    elif left_text in ['ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°', 'ä½œå‡ºå†³å®šæœºå…³', 'å†³å®šæœºå…³', 'æœºå…³åç§°', 'ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³']:
                        data['ä½œå‡ºå†³å®šæœºå…³'] = right_text
                    elif left_text in ['ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ', 'ä½œå‡ºå†³å®šæ—¥æœŸ', 'å†³å®šæ—¥æœŸ', 'å¤„ç½šå†³å®šæ—¥æœŸ']:
                        data['ä½œå‡ºå†³å®šæ—¥æœŸ'] = right_text
                    elif left_text in ['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·', 'å†³å®šä¹¦æ–‡å·']:
                        data['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·'] = right_text
                    # åŒ…å«åŒ¹é…ï¼ˆä½œä¸ºåå¤‡ï¼‰
                    elif 'åºå·' in left_text:
                        data['åºå·'] = right_text
                    elif any(keyword in left_text for keyword in ['å½“äº‹äºº', 'è¢«å¤„ç½šå½“äº‹äºº']):
                        data['å½“äº‹äººåç§°'] = right_text
                    elif any(keyword in left_text for keyword in ['è¿æ³•è¿è§„äº‹å®', 'è¿æ³•è¿è§„è¡Œä¸º', 'ä¸»è¦è¿æ³•è¿è§„', 'è¿æ³•è¡Œä¸º']):
                        data['ä¸»è¦è¿æ³•è¿è§„è¡Œä¸º'] = right_text
                    elif 'è¡Œæ”¿å¤„ç½šä¾æ®' in left_text or 'å¤„ç½šä¾æ®' in left_text:
                        data['è¡Œæ”¿å¤„ç½šä¾æ®'] = right_text
                    elif 'è¡Œæ”¿å¤„ç½šå†³å®š' in left_text and 'æœºå…³' not in left_text and 'æ—¥æœŸ' not in left_text:
                        data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = right_text
                    elif 'å¤„ç½šå†…å®¹' in left_text:
                        data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = right_text
                    elif any(keyword in left_text for keyword in ['ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³', 'å†³å®šæœºå…³', 'æœºå…³åç§°']) and 'æ—¥æœŸ' not in left_text:
                        data['ä½œå‡ºå†³å®šæœºå…³'] = right_text
                    elif any(keyword in left_text for keyword in ['ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ', 'å†³å®šæ—¥æœŸ', 'å¤„ç½šå†³å®šæ—¥æœŸ']):
                        data['ä½œå‡ºå†³å®šæ—¥æœŸ'] = right_text
                    elif any(keyword in left_text for keyword in ['å†³å®šä¹¦æ–‡å·', 'è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·']):
                        data['è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·'] = right_text
                    else:
                        # ä¿å­˜å…¶ä»–å­—æ®µ
                        data[left_text] = right_text
            
            self.logger.debug(f"é”®å€¼å¯¹è¡¨æ ¼è§£æç»“æœ: {data}")
            return data
            
        except Exception as e:
            self.logger.error(f"è§£æé”®å€¼å¯¹è¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def process_link_with_new_window(self, href: str, title: str) -> Dict:
        """åœ¨æ–°çª—å£ä¸­å¤„ç†é“¾æ¥ - å‚è€ƒç”¨æˆ·ä»£ç çš„çª—å£å¤„ç†æ–¹å¼"""
        try:
            self.logger.info(f"æ­£åœ¨å¤„ç†: {title}")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # åœ¨æ–°çª—å£ä¸­æ‰“å¼€é“¾æ¥
            self.driver.execute_script("window.open(arguments[0], '_blank');", href)
            new_window = self.driver.window_handles[-1]
            original_window = self.driver.window_handles[0]
            
            # åˆ‡æ¢åˆ°æ–°çª—å£
            self.driver.switch_to.window(new_window)
            
            try:
                # ç­‰å¾…é¡µé¢åŠ è½½
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                time.sleep(2)
                
                # æå–å‘å¸ƒæ—¶é—´
                publish_time = self.extract_publish_time()
                
                # æŸ¥æ‰¾è¡¨æ ¼
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                # æ”¯æŒå¤šç§è¡¨æ ¼ç±»å‹
                tables = soup.find_all('table', class_=['MsoTableGrid', 'MsoNormalTable'])
                
                if not tables:
                    # å¦‚æœæ²¡æ‰¾åˆ°æŒ‡å®šç±»çš„è¡¨æ ¼ï¼ŒæŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
                    tables = soup.find_all('table')
                
                detail_data = {}
                for table in tables:
                    # è§£æè¡¨æ ¼å†…å®¹
                    table_data = self.parse_table_from_soup(table)
                    if table_data:
                        detail_data.update(table_data)
                        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæœ‰æ•ˆè¡¨æ ¼
                
                if detail_data:
                    # å¤„ç†å¤šè®°å½•æƒ…å†µ
                    result_records = []
                    
                    # æ·»åŠ åŸºç¡€ä¿¡æ¯
                    base_info = {
                        'æŠ“å–æ—¶é—´': get_current_timestamp(),
                        'è¯¦æƒ…é“¾æ¥': href,
                        'æ ‡é¢˜': title,
                        'å‘å¸ƒæ—¶é—´': publish_time  # ç¡®ä¿å‘å¸ƒæ—¶é—´è¢«åŒ…å«
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰additional_recordsï¼ˆå¤šè®°å½•æ‰¹æ–‡ï¼‰
                    if 'additional_records' in detail_data:
                        additional_records = detail_data.pop('additional_records')
                        
                        # ä¸»è®°å½•
                        main_record = {**detail_data, **base_info}
                        result_records.append(main_record)
                        
                        # é™„åŠ è®°å½•
                        for add_record in additional_records:
                            # ç»§æ‰¿ä¸»è®°å½•çš„å…±åŒä¿¡æ¯ï¼ˆå¦‚å†³å®šæœºå…³ã€æ ‡é¢˜ç­‰ï¼‰
                            combined_record = {**base_info}
                            
                            # æ·»åŠ é™„åŠ è®°å½•çš„ç‰¹å®šä¿¡æ¯
                            combined_record.update(add_record)
                            
                            # ç»§æ‰¿ä¸»è®°å½•ä¸­çš„å…±åŒå­—æ®µï¼ˆå¦‚æœé™„åŠ è®°å½•ä¸­æ²¡æœ‰ï¼‰
                            for key in ['ä½œå‡ºå†³å®šæœºå…³', 'è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·', 'æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´']:
                                if key in detail_data and (key not in combined_record or not combined_record[key]):
                                    combined_record[key] = detail_data[key]
                            
                            result_records.append(combined_record)
                        
                        self.logger.info(f"æˆåŠŸè§£æå¤šè®°å½•å¤„ç½šä¿¡æ¯: {title}ï¼Œå…±{len(result_records)}æ¡è®°å½•")
                        
                        # è¿”å›å¤šè®°å½•æ ‡è¯†
                        return {
                            'is_multi_record': True,
                            'records': result_records,
                            'total_count': len(result_records)
                        }
                    else:
                        # å•è®°å½•æƒ…å†µ
                        detail_data.update(base_info)
                        self.logger.info(f"æˆåŠŸè§£æå¤„ç½šä¿¡æ¯: {title}")
                        return detail_data
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆè¡¨æ ¼æ•°æ®: {title}")
                
                return detail_data
                
            finally:
                # å…³é—­æ–°çª—å£å¹¶åˆ‡æ¢å›åŸçª—å£
                self.driver.close()
                self.driver.switch_to.window(original_window)
                
        except Exception as e:
            self.logger.error(f"å¤„ç†é“¾æ¥å¤±è´¥ {href}: {e}")
            try:
                # ç¡®ä¿åˆ‡æ¢å›åŸçª—å£
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return {}
    
    def parse_table_from_soup(self, table) -> Dict:
        """ä»BeautifulSoupè¡¨æ ¼å¯¹è±¡è§£ææ•°æ® - å¢å¼ºç‰ˆæœ¬æ”¯æŒå¤šç§è¡¨æ ¼æ ¼å¼"""
        try:
            # é¦–å…ˆå°è¯•æå–å®Œæ•´çš„é¡µé¢æ–‡æœ¬å†…å®¹ç”¨äºå¤‡ç”¨è§£æ
            page_text = clean_text(table.get_text())
            self.logger.debug(f"è¡¨æ ¼æ–‡æœ¬å†…å®¹é•¿åº¦: {len(page_text)}")
            
            # å°è¯•ä»å®Œæ•´æ–‡æœ¬ä¸­æå–è¡Œæ”¿å¤„ç½šä¾æ®
            punishment_basis = self.extract_punishment_basis_from_text(page_text)
            
            # åˆå§‹åŒ–æ•°æ®å­—å…¸
            data = {}
            if punishment_basis:
                data['è¡Œæ”¿å¤„ç½šä¾æ®'] = punishment_basis
            
            rows = table.find_all('tr')
            
            if len(rows) < 2:
                return data if data else {}
            
            # æ£€æŸ¥è¡¨æ ¼ç±»å‹ï¼šæ¨ªå‘å¤šåˆ— vs é”®å€¼å¯¹
            first_row = rows[0]
            first_row_cells = first_row.find_all(['td', 'th'])
            
            # å¦‚æœç¬¬ä¸€è¡Œæœ‰å¤šä¸ªåˆ—ï¼ˆ>=3ï¼‰ï¼Œä¸”åŒ…å«å¸¸è§çš„è¡¨å¤´å…³é”®è¯ï¼Œåˆ™ä¸ºæ¨ªå‘è¡¨æ ¼
            if len(first_row_cells) >= 3:
                header_texts = [clean_text(cell.get_text()) for cell in first_row_cells]
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„è¡¨å¤´å…³é”®è¯
                header_keywords = ['åºå·', 'å½“äº‹äºº', 'è¿æ³•', 'å¤„ç½š', 'æœºå…³']
                if any(any(keyword in header for keyword in header_keywords) for header in header_texts):
                    self.logger.info("æ£€æµ‹åˆ°æ¨ªå‘å¤šåˆ—è¡¨æ ¼ï¼Œä½¿ç”¨æ¨ªå‘è§£æé€»è¾‘")
                    table_data = self.parse_horizontal_table(rows)
                    data.update(table_data)
                    return data
            
            # å¦åˆ™ä½¿ç”¨é”®å€¼å¯¹è§£æé€»è¾‘
            self.logger.info("æ£€æµ‹åˆ°é”®å€¼å¯¹è¡¨æ ¼ï¼Œä½¿ç”¨é”®å€¼å¯¹è§£æé€»è¾‘")
            table_data = self.parse_key_value_table(rows)
            data.update(table_data)
            
            # å¦‚æœé”®å€¼å¯¹è§£ææ²¡æœ‰æ‰¾åˆ°å¤„ç½šå†…å®¹ï¼Œå°è¯•ä»å®Œæ•´æ–‡æœ¬ä¸­æå–
            if 'è¡Œæ”¿å¤„ç½šå†…å®¹' not in data or not data['è¡Œæ”¿å¤„ç½šå†…å®¹']:
                punishment_content = self.extract_punishment_content_from_text(page_text)
                if punishment_content:
                    data['è¡Œæ”¿å¤„ç½šå†…å®¹'] = punishment_content
            
            return data
            
        except Exception as e:
            self.logger.error(f"è§£æè¡¨æ ¼å¤±è´¥: {e}")
            return {}
    
    def crawl_category_smart(self, category: str, target_year: int = None, target_month: int = None, max_pages: int = 10, max_records: int = None, use_smart_check: bool = False) -> List[Dict]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«çš„å¤„ç½šä¿¡æ¯ - æ”¯æŒæŒ‰æœˆä»½è¿‡æ»¤"""
        self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} å¤„ç½šä¿¡æ¯")
        
        if target_year and target_month:
            self.logger.info(f"ç›®æ ‡æœˆä»½: {target_year}å¹´{target_month}æœˆ")
        
        # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•è·å–å¤„ç½šåˆ—è¡¨
        punishment_list = self.get_punishment_list_smart(category, target_year, target_month, max_pages, use_smart_check)
        
        if not punishment_list:
            self.logger.warning(f"{category} æ²¡æœ‰æ‰¾åˆ°å¤„ç½šä¿¡æ¯")
            return []
        
        # å¦‚æœè®¾ç½®äº†max_recordsï¼Œé™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡
        if max_records and len(punishment_list) > max_records:
            self.logger.info(f"{category} æ‰¾åˆ° {len(punishment_list)} æ¡è®°å½•ï¼Œé™åˆ¶å¤„ç†å‰ {max_records} æ¡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            punishment_list = punishment_list[:max_records]
        
        # è·å–è¯¦æƒ…ä¿¡æ¯
        detailed_data = []
        for i, item in enumerate(punishment_list, 1):
            self.logger.info(f"æ­£åœ¨å¤„ç† {category} ç¬¬ {i}/{len(punishment_list)} æ¡è®°å½•")
            
            detail_url = item.get('detail_url')
            if not detail_url:
                self.logger.warning(f"ç¬¬ {i} æ¡è®°å½•ç¼ºå°‘è¯¦æƒ…é“¾æ¥")
                continue
            
            # ä½¿ç”¨æ–°çª—å£å¤„ç†æ–¹å¼
            detail_data = self.process_link_with_new_window(detail_url, item.get('title', ''))
            
            if detail_data:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè®°å½•æ‰¹æ–‡
                if isinstance(detail_data, dict) and detail_data.get('is_multi_record'):
                    # å¤šè®°å½•æƒ…å†µï¼šå±•å¼€æ‰€æœ‰è®°å½•
                    records = detail_data.get('records', [])
                    for record in records:
                        # åˆå¹¶åˆ—è¡¨ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                        combined_data = {**item, **record}
                        detailed_data.append(combined_data)
                    
                    self.logger.info(f"å¤šè®°å½•æ‰¹æ–‡å¤„ç†å®Œæˆï¼Œå±•å¼€ä¸º{len(records)}æ¡ç‹¬ç«‹è®°å½•")
                else:
                    # å•è®°å½•æƒ…å†µ
                    combined_data = {**item, **detail_data}
                    detailed_data.append(combined_data)
            
            # è¯·æ±‚é—´éš”
            time.sleep(CRAWL_CONFIG['delay_between_requests'])
        
        self.logger.info(f"{category} å¤„ç½šä¿¡æ¯çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(detailed_data)} æ¡è¯¦ç»†è®°å½•")
        return detailed_data

    def crawl_category(self, category: str, max_pages: int = 5, max_records: int = None) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šç±»åˆ«çš„æ‰€æœ‰å¤„ç½šä¿¡æ¯"""
        self.logger.info(f"å¼€å§‹çˆ¬å– {category} å¤„ç½šä¿¡æ¯")
        
        # è·å–å¤„ç½šåˆ—è¡¨
        punishment_list = self.get_punishment_list(category, max_pages)
        
        if not punishment_list:
            self.logger.warning(f"{category} æ²¡æœ‰æ‰¾åˆ°å¤„ç½šä¿¡æ¯")
            return []
        
        # å¦‚æœè®¾ç½®äº†max_recordsï¼Œé™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡
        if max_records and len(punishment_list) > max_records:
            self.logger.info(f"{category} æ‰¾åˆ° {len(punishment_list)} æ¡è®°å½•ï¼Œé™åˆ¶å¤„ç†å‰ {max_records} æ¡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            punishment_list = punishment_list[:max_records]
        
        # è·å–è¯¦æƒ…ä¿¡æ¯
        detailed_data = []
        for i, item in enumerate(punishment_list, 1):
            self.logger.info(f"æ­£åœ¨å¤„ç† {category} ç¬¬ {i}/{len(punishment_list)} æ¡è®°å½•")
            
            detail_url = item.get('detail_url')
            if not detail_url:
                self.logger.warning(f"ç¬¬ {i} æ¡è®°å½•ç¼ºå°‘è¯¦æƒ…é“¾æ¥")
                continue
            
            # ä½¿ç”¨æ–°çª—å£å¤„ç†æ–¹å¼
            detail_data = self.process_link_with_new_window(detail_url, item.get('title', ''))
            
            if detail_data:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè®°å½•æ‰¹æ–‡
                if isinstance(detail_data, dict) and detail_data.get('is_multi_record'):
                    # å¤šè®°å½•æƒ…å†µï¼šå±•å¼€æ‰€æœ‰è®°å½•
                    records = detail_data.get('records', [])
                    for record in records:
                        # åˆå¹¶åˆ—è¡¨ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                        combined_data = {**item, **record}
                        detailed_data.append(combined_data)
                    
                    self.logger.info(f"å¤šè®°å½•æ‰¹æ–‡å¤„ç†å®Œæˆï¼Œå±•å¼€ä¸º{len(records)}æ¡ç‹¬ç«‹è®°å½•")
                else:
                    # å•è®°å½•æƒ…å†µ
                    combined_data = {**item, **detail_data}
                    detailed_data.append(combined_data)
            
            # è¯·æ±‚é—´éš”
            time.sleep(CRAWL_CONFIG['delay_between_requests'])
        
        self.logger.info(f"{category} å¤„ç½šä¿¡æ¯çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(detailed_data)} æ¡è¯¦ç»†è®°å½•")
        return detailed_data
    
    def crawl_all_smart(self, target_year: int = None, target_month: int = None, max_pages_per_category: int = 10, max_records_per_category: int = None, use_smart_check: bool = False) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æ‰€æœ‰ç±»åˆ«çš„å¤„ç½šä¿¡æ¯ - æ”¯æŒæŒ‰æœˆä»½è¿‡æ»¤"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        # æ‰“å°æ™ºèƒ½çˆ¬å–ä¿¡æ¯
        if target_year and target_month:
            self.logger.info(f"å¯ç”¨æ™ºèƒ½çˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡æœˆä»½: {target_year}å¹´{target_month}æœˆ")
        
        try:
            for category in BASE_URLS.keys():
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category}")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–
                category_data = self.crawl_category_smart(
                    category, 
                    target_year, 
                    target_month, 
                    max_pages_per_category, 
                    max_records_per_category,
                    use_smart_check
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡è®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°ç›®æ ‡æœˆä»½çš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            if target_year and target_month:
                self.logger.info(f"æ™ºèƒ½çˆ¬å–å®Œæˆï¼Œ{target_year}å¹´{target_month}æœˆå…±è·å¾— {total_records} æ¡è®°å½•")
            else:
                self.logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_all(self, max_pages_per_category: int = 5, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """çˆ¬å–æ‰€æœ‰ç±»åˆ«çš„å¤„ç½šä¿¡æ¯"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        try:
            for category in BASE_URLS.keys():
                self.logger.info(f"å¼€å§‹çˆ¬å– {category}")
                category_data = self.crawl_category(category, max_pages_per_category, max_records_per_category)
                all_data[category] = category_data
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            self.logger.info("æ‰€æœ‰ç±»åˆ«çˆ¬å–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def extract_publish_time(self) -> str:
        """æå–é¡µé¢å‘å¸ƒæ—¶é—´"""
        try:
            # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´å…ƒç´  - æ”¯æŒå¤šç§æ ¼å¼
            time_selectors = [
                'span.ng-binding',  # ä½ æä¾›çš„æ ¼å¼
                'span[ng-bind*="time"]',
                '.publish-time',
                '.pub-time',
                '.time',
                '*[class*="time"]'
            ]
            
            for selector in time_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if 'å‘å¸ƒæ—¶é—´' in text or 'æ—¶é—´' in text:
                            # æå–æ—¶é—´éƒ¨åˆ†ï¼Œæ”¯æŒå¤šç§æ ¼å¼
                            import re
                            # åŒ¹é… YYYY-MM-DD æ ¼å¼
                            time_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', text)
                            if time_match:
                                self.logger.debug(f"æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {time_match.group(1)}")
                                return time_match.group(1)
                except:
                    continue
            
            # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„å‘å¸ƒæ—¶é—´å…ƒç´ ï¼Œå°è¯•ä»é¡µé¢å†…å®¹ä¸­æå–
            try:
                page_source = self.driver.page_source
                import re
                # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´ç›¸å…³çš„æ¨¡å¼
                patterns = [
                    r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                    r'æ—¶é—´[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                    r'æ—¥æœŸ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, page_source)
                    if match:
                        self.logger.debug(f"ä»é¡µé¢æºç æå–å‘å¸ƒæ—¶é—´: {match.group(1)}")
                        return match.group(1)
                        
            except Exception as e:
                self.logger.debug(f"ä»é¡µé¢æºç æå–æ—¶é—´å¤±è´¥: {e}")
            
            self.logger.debug("æœªæ‰¾åˆ°å‘å¸ƒæ—¶é—´ä¿¡æ¯")
            return ""
            
        except Exception as e:
            self.logger.warning(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return ""

    def crawl_all_smart_by_year(self, target_year: int, max_pages_per_category: int = 50, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å¤„ç½šä¿¡æ¯ - æ”¯æŒæŒ‰å¹´ä»½è¿‡æ»¤"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½å¹´ä»½çˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡å¹´ä»½: {target_year}å¹´")
        
        try:
            for category in BASE_URLS.keys():
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} - {target_year}å¹´æ•°æ®")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æŒ‡å®šå¹´ä»½
                category_data = self.crawl_category_smart_by_year(
                    category, 
                    target_year, 
                    max_pages_per_category, 
                    max_records_per_category
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡{target_year}å¹´è®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°{target_year}å¹´çš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            self.logger.info(f"æ™ºèƒ½å¹´ä»½çˆ¬å–å®Œæˆï¼Œ{target_year}å¹´å…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½å¹´ä»½çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_category_smart_by_year(self, category: str, target_year: int, max_pages: int = 50, max_records: int = None) -> List[Dict]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«æŒ‡å®šå¹´ä»½çš„å¤„ç½šä¿¡æ¯"""
        self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} {target_year}å¹´å¤„ç½šä¿¡æ¯")
        
        # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•è·å–å¤„ç½šåˆ—è¡¨ï¼ŒæŒ‰å¹´ä»½è¿‡æ»¤
        punishment_list = self.get_punishment_list_smart_by_year(category, target_year, max_pages)
        
        if not punishment_list:
            self.logger.warning(f"{category} æ²¡æœ‰æ‰¾åˆ°{target_year}å¹´å¤„ç½šä¿¡æ¯")
            return []
        
        # å¦‚æœè®¾ç½®äº†max_recordsï¼Œé™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡
        if max_records and len(punishment_list) > max_records:
            self.logger.info(f"{category} æ‰¾åˆ° {len(punishment_list)} æ¡è®°å½•ï¼Œé™åˆ¶å¤„ç†å‰ {max_records} æ¡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            punishment_list = punishment_list[:max_records]
        
        # è·å–è¯¦æƒ…ä¿¡æ¯
        detailed_data = []
        for i, item in enumerate(punishment_list, 1):
            self.logger.info(f"æ­£åœ¨å¤„ç† {category} ç¬¬ {i}/{len(punishment_list)} æ¡è®°å½•")
            
            detail_url = item.get('detail_url')
            if not detail_url:
                self.logger.warning(f"ç¬¬ {i} æ¡è®°å½•ç¼ºå°‘è¯¦æƒ…é“¾æ¥")
                continue
            
            # ä½¿ç”¨æ–°çª—å£å¤„ç†æ–¹å¼
            detail_data = self.process_link_with_new_window(detail_url, item.get('title', ''))
            
            if detail_data:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè®°å½•æ‰¹æ–‡
                if isinstance(detail_data, dict) and detail_data.get('is_multi_record'):
                    # å¤šè®°å½•æƒ…å†µï¼šå±•å¼€æ‰€æœ‰è®°å½•
                    records = detail_data.get('records', [])
                    for record in records:
                        # åˆå¹¶åˆ—è¡¨ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                        combined_data = {**item, **record}
                        detailed_data.append(combined_data)
                    
                    self.logger.info(f"å¤šè®°å½•æ‰¹æ–‡å¤„ç†å®Œæˆï¼Œå±•å¼€ä¸º{len(records)}æ¡ç‹¬ç«‹è®°å½•")
                else:
                    # å•è®°å½•æƒ…å†µ
                    combined_data = {**item, **detail_data}
                    detailed_data.append(combined_data)
            
            # è¯·æ±‚é—´éš”
            time.sleep(CRAWL_CONFIG['delay_between_requests'])
        
        self.logger.info(f"{category} {target_year}å¹´å¤„ç½šä¿¡æ¯çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(detailed_data)} æ¡è¯¦ç»†è®°å½•")
        return detailed_data

    def get_punishment_list_smart_by_year(self, category: str, target_year: int, max_pages: int = 50) -> List[Dict]:
        """æ™ºèƒ½è·å–æŒ‡å®šå¹´ä»½çš„å¤„ç½šä¿¡æ¯åˆ—è¡¨"""
        url = BASE_URLS.get(category)
        if not url:
            self.logger.error(f"æœªæ‰¾åˆ°ç±»åˆ« '{category}' å¯¹åº”çš„URL")
            return []
        
        if not self.load_page_with_retry(url):
            self.logger.error(f"æ— æ³•åŠ è½½ {category} é¡µé¢")
            return []
        
        all_punishment_list = []
        current_page = 1
        found_target_year = False
        previous_page_latest_date = None
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½å¹´ä»½æ£€æŸ¥: {target_year}å¹´")
        target_year_str = str(target_year)
        
        try:
            while current_page <= max_pages:
                self.logger.info(f"æ­£åœ¨è§£æ {category} ç¬¬ {current_page} é¡µ")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œä¼˜åŒ–ç­‰å¾…æ—¶é—´
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(0.5)  # å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ä»2ç§’åˆ°0.5ç§’
                
                # æ™ºèƒ½æ£€æŸ¥ï¼šè·å–å½“å‰é¡µé¢çš„å‘å¸ƒæ—¶é—´
                publish_dates = self.get_page_publish_dates()
                current_page_latest_date = publish_dates[0] if publish_dates else None
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å¹´ä»½
                page_has_target_year = any(date.startswith(target_year_str) for date in publish_dates)
                
                if page_has_target_year:
                    found_target_year = True
                    self.logger.info(f"ç¬¬ {current_page} é¡µåŒ…å«ç›®æ ‡å¹´ä»½ {target_year} æ•°æ®ï¼Œç»§ç»­çˆ¬å–")
                else:
                    # æ²¡æœ‰ç›®æ ‡å¹´ä»½æ•°æ®çš„å¤„ç†é€»è¾‘
                    if found_target_year:
                        # ä¹‹å‰æ‰¾åˆ°è¿‡ç›®æ ‡å¹´ä»½ï¼Œç°åœ¨æ²¡æœ‰äº†ï¼Œè¯´æ˜å·²ç»è¶…è¿‡ç›®æ ‡å¹´ä»½èŒƒå›´
                        self.logger.info(f"ç¬¬ {current_page} é¡µä¸å†åŒ…å«ç›®æ ‡å¹´ä»½æ•°æ®ï¼Œç›®æ ‡å¹´ä»½èŒƒå›´å·²ç»“æŸï¼Œåœæ­¢çˆ¬å–")
                        break
                    elif current_page == 1:
                        # ç¬¬1é¡µæ²¡æœ‰ç›®æ ‡å¹´ä»½æ•°æ®
                        if current_page_latest_date and current_page_latest_date > target_year_str:
                            self.logger.info(f"ç¬¬1é¡µæœ€æ–°æ•°æ®({current_page_latest_date})æ¯”ç›®æ ‡å¹´ä»½({target_year})æ–°ï¼Œè¯¥åˆ†ç±»æ— ç›®æ ‡å¹´ä»½æ•°æ®")
                            break
                        else:
                            self.logger.info(f"ç¬¬1é¡µæš‚æ— ç›®æ ‡å¹´ä»½æ•°æ®ï¼Œæ£€æŸ¥ç¬¬2é¡µç¡®è®¤")
                    elif current_page == 2:
                        # ç¬¬2é¡µçš„å®‰å…¨æ£€æŸ¥
                        if (previous_page_latest_date and current_page_latest_date and 
                            current_page_latest_date < previous_page_latest_date):
                            # æ£€æŸ¥æ˜¯å¦æ•°æ®è¿‡è€ï¼ˆæ¯”ç›®æ ‡å¹´ä»½å°å¾ˆå¤šï¼‰
                            if current_page_latest_date and current_page_latest_date < target_year_str:
                                self.logger.info(f"ç¬¬2é¡µæ•°æ®({current_page_latest_date})æ¯”ç›®æ ‡å¹´ä»½({target_year})æ›´è€ä¸”æ— ç›®æ ‡å¹´ä»½ï¼Œåœæ­¢ç¿»é¡µ")
                                break
                            else:
                                self.logger.info(f"ç¬¬2é¡µæ•°æ®({current_page_latest_date})æ¯”ç¬¬1é¡µ({previous_page_latest_date})æ›´è€ä½†å¹´ä»½æ¥è¿‘ï¼Œç»§ç»­æŸ¥æ‰¾")
                        else:
                            self.logger.info(f"ç¬¬2é¡µæš‚æ— ç›®æ ‡å¹´ä»½æ•°æ®ï¼Œç»§ç»­æŸ¥æ‰¾")
                    else:
                        # ç¬¬3é¡µåŠä»¥åï¼Œå¦‚æœè¿˜æ²¡æ‰¾åˆ°ç›®æ ‡å¹´ä»½å°±åœæ­¢
                        if current_page_latest_date and current_page_latest_date < target_year_str:
                            self.logger.info(f"ç¬¬{current_page}é¡µæ•°æ®({current_page_latest_date})æ¯”ç›®æ ‡å¹´ä»½({target_year})æ›´è€ï¼Œåœæ­¢æŸ¥æ‰¾")
                            break
                        else:
                            self.logger.info(f"ç¬¬{current_page}é¡µä»æ— ç›®æ ‡å¹´ä»½æ•°æ®ï¼Œç»§ç»­æŸ¥æ‰¾")
                    
                    # è®°å½•å½“å‰é¡µæœ€æ–°æ—¥æœŸï¼Œç”¨äºä¸‹ä¸€é¡µæ¯”è¾ƒ
                    previous_page_latest_date = current_page_latest_date
                    
                    # æ²¡æœ‰ç›®æ ‡å¹´ä»½æ•°æ®ï¼Œç¿»åˆ°ä¸‹ä¸€é¡µç»§ç»­æ£€æŸ¥
                    if current_page < max_pages:
                        try:
                            next_buttons = self.driver.find_elements(
                                By.XPATH, 
                                '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                            )
                            
                            if next_buttons:
                                next_button = next_buttons[0]
                                if next_button.is_enabled():
                                    self.driver.execute_script("arguments[0].click();", next_button)
                                    current_page += 1
                                    time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                    continue
                                else:
                                    self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                    break
                            else:
                                self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                                break
                        except Exception as e:
                            self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                            break
                    else:
                        break
                
                # è§£æå½“å‰é¡µé¢çš„å¤„ç½šä¿¡æ¯
                try:
                    # æŸ¥æ‰¾åŒ…å«"è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€è¡¨"çš„é“¾æ¥
                    punishment_links = self.driver.find_elements(
                        By.XPATH, 
                        '//a[contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬ç¤º") or contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€") or contains(text(), "å¤„ç½šä¿¡æ¯")]'
                    )
                    
                    if not punishment_links:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é“¾æ¥æ¨¡å¼
                        punishment_links = self.driver.find_elements(
                            By.XPATH, 
                            '//a[contains(@href, "ItemDetail")]'
                        )
                    
                    page_punishment_list = []
                    for link in punishment_links:
                        try:
                            href = link.get_attribute('href')
                            title = clean_text(link.text)
                            
                            if href and title:
                                # æ„å»ºå®Œæ•´URL
                                if not href.startswith('http'):
                                    href = urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                
                                punishment_info = {
                                    'title': title,
                                    'detail_url': href,
                                    'category': category,
                                    'page': current_page
                                }
                                page_punishment_list.append(punishment_info)
                                
                        except Exception as e:
                            self.logger.warning(f"è§£æé“¾æ¥å¤±è´¥: {e}")
                            continue
                    
                    self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(page_punishment_list)} æ¡å¤„ç½šä¿¡æ¯")
                    all_punishment_list.extend(page_punishment_list)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    if current_page < max_pages:
                        try:
                            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                            next_buttons = self.driver.find_elements(
                                By.XPATH, 
                                '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                            )
                            
                            if next_buttons:
                                next_button = next_buttons[0]
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                                if next_button.is_enabled():
                                    self.driver.execute_script("arguments[0].click();", next_button)
                                    current_page += 1
                                    time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                else:
                                    self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                    break
                            else:
                                self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                                
                        except Exception as e:
                            self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                            break
                    else:
                        break
                        
                except Exception as e:
                    self.logger.error(f"è§£æç¬¬ {current_page} é¡µå¤±è´¥: {e}")
                    break
            
            # æ™ºèƒ½æ£€æŸ¥ç»“æœåé¦ˆ
            if not found_target_year:
                self.logger.info(f"{category} æ™ºèƒ½æ£€æŸ¥å®Œæˆï¼Œæœªæ‰¾åˆ° {target_year}å¹´ çš„æ•°æ®")
            
            self.logger.info(f"{category} å¤„ç½šåˆ—è¡¨è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_punishment_list)} æ¡è®°å½•")
            return all_punishment_list
            
        except Exception as e:
            self.logger.error(f"è§£æ {category} å¤„ç½šåˆ—è¡¨å¤±è´¥: {e}")
            return all_punishment_list

    def crawl_all_smart_by_date(self, target_year: int, target_month: int, target_day: int, max_pages_per_category: int = 3, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å¤„ç½šä¿¡æ¯ - æ”¯æŒæŒ‰æ—¥æœŸè¿‡æ»¤"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        target_date_str = f"{target_year}-{target_month:02d}-{target_day:02d}"
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½æ—¥æœŸçˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡æ—¥æœŸ: {target_date_str}")
        
        try:
            for category in BASE_URLS.keys():
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} - {target_date_str}æ•°æ®")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æŒ‡å®šæ—¥æœŸ
                category_data = self.crawl_category_smart_by_date(
                    category, 
                    target_year, 
                    target_month, 
                    target_day,
                    max_pages_per_category, 
                    max_records_per_category
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡{target_date_str}è®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°{target_date_str}çš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            self.logger.info(f"æ™ºèƒ½æ—¥æœŸçˆ¬å–å®Œæˆï¼Œ{target_date_str}å…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ—¥æœŸçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_category_smart_by_date(self, category: str, target_year: int, target_month: int, target_day: int, max_pages: int = 3, max_records: int = None) -> List[Dict]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«æŒ‡å®šæ—¥æœŸçš„å¤„ç½šä¿¡æ¯"""
        target_date_str = f"{target_year}-{target_month:02d}-{target_day:02d}"
        self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} {target_date_str}å¤„ç½šä¿¡æ¯")
        
        # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•è·å–å¤„ç½šåˆ—è¡¨ï¼ŒæŒ‰æ—¥æœŸè¿‡æ»¤
        punishment_list = self.get_punishment_list_smart_by_date(category, target_year, target_month, target_day, max_pages)
        
        if not punishment_list:
            self.logger.warning(f"{category} æ²¡æœ‰æ‰¾åˆ°{target_date_str}å¤„ç½šä¿¡æ¯")
            return []
        
        # å¦‚æœè®¾ç½®äº†max_recordsï¼Œé™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡
        if max_records and len(punishment_list) > max_records:
            self.logger.info(f"{category} æ‰¾åˆ° {len(punishment_list)} æ¡è®°å½•ï¼Œé™åˆ¶å¤„ç†å‰ {max_records} æ¡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            punishment_list = punishment_list[:max_records]
        
        # è·å–è¯¦æƒ…ä¿¡æ¯
        detailed_data = []
        for i, item in enumerate(punishment_list, 1):
            self.logger.info(f"æ­£åœ¨å¤„ç† {category} ç¬¬ {i}/{len(punishment_list)} æ¡è®°å½•")
            
            detail_url = item.get('detail_url')
            if not detail_url:
                self.logger.warning(f"ç¬¬ {i} æ¡è®°å½•ç¼ºå°‘è¯¦æƒ…é“¾æ¥")
                continue
            
            # ä½¿ç”¨æ–°çª—å£å¤„ç†æ–¹å¼
            detail_data = self.process_link_with_new_window(detail_url, item.get('title', ''))
            
            if detail_data:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè®°å½•æ‰¹æ–‡
                if isinstance(detail_data, dict) and detail_data.get('is_multi_record'):
                    # å¤šè®°å½•æƒ…å†µï¼šå±•å¼€æ‰€æœ‰è®°å½•
                    records = detail_data.get('records', [])
                    for record in records:
                        # åˆå¹¶åˆ—è¡¨ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                        combined_data = {**item, **record}
                        detailed_data.append(combined_data)
                    
                    self.logger.info(f"å¤šè®°å½•æ‰¹æ–‡å¤„ç†å®Œæˆï¼Œå±•å¼€ä¸º{len(records)}æ¡ç‹¬ç«‹è®°å½•")
                else:
                    # å•è®°å½•æƒ…å†µ
                    combined_data = {**item, **detail_data}
                    detailed_data.append(combined_data)
            
            # è¯·æ±‚é—´éš”
            time.sleep(CRAWL_CONFIG['delay_between_requests'])
        
        self.logger.info(f"{category} {target_date_str}å¤„ç½šä¿¡æ¯çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(detailed_data)} æ¡è¯¦ç»†è®°å½•")
        return detailed_data

    def get_punishment_list_smart_by_date(self, category: str, target_year: int, target_month: int, target_day: int, max_pages: int = 3) -> List[Dict]:
        """æ™ºèƒ½è·å–æŒ‡å®šæ—¥æœŸçš„å¤„ç½šä¿¡æ¯åˆ—è¡¨"""
        url = BASE_URLS.get(category)
        if not url:
            self.logger.error(f"æœªæ‰¾åˆ°ç±»åˆ« '{category}' å¯¹åº”çš„URL")
            return []
        
        if not self.load_page_with_retry(url):
            self.logger.error(f"æ— æ³•åŠ è½½ {category} é¡µé¢")
            return []
        
        all_punishment_list = []
        current_page = 1
        found_target_date = False
        target_date_str = f"{target_year}-{target_month:02d}-{target_day:02d}"
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½æ—¥æœŸæ£€æŸ¥: {target_date_str}")
        
        try:
            while current_page <= max_pages:
                self.logger.info(f"æ­£åœ¨è§£æ {category} ç¬¬ {current_page} é¡µ")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œä¼˜åŒ–ç­‰å¾…æ—¶é—´
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(0.5)  # å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ä»2ç§’åˆ°0.5ç§’
                
                # æ™ºèƒ½æ£€æŸ¥ï¼šè·å–å½“å‰é¡µé¢çš„å‘å¸ƒæ—¶é—´
                publish_dates = self.get_page_publish_dates()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸ
                page_has_target_date = any(date.startswith(target_date_str) for date in publish_dates)
                
                if page_has_target_date:
                    found_target_date = True
                    self.logger.info(f"ç¬¬ {current_page} é¡µåŒ…å«ç›®æ ‡æ—¥æœŸ {target_date_str} æ•°æ®ï¼Œå¼€å§‹çˆ¬å–")
                    
                    # è§£æå½“å‰é¡µé¢ï¼Œåªè·å–ç›®æ ‡æ—¥æœŸçš„è®°å½•
                    try:
                        page_punishment_list = []
                        
                        # æŸ¥æ‰¾åŒ…å«"è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€è¡¨"çš„é“¾æ¥
                        punishment_links = self.driver.find_elements(
                            By.XPATH, 
                            '//a[contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬ç¤º") or contains(text(), "è¡Œæ”¿å¤„ç½šä¿¡æ¯å…¬å¼€") or contains(text(), "å¤„ç½šä¿¡æ¯")]'
                        )
                        
                        if not punishment_links:
                            # å°è¯•å…¶ä»–å¯èƒ½çš„é“¾æ¥æ¨¡å¼
                            punishment_links = self.driver.find_elements(
                                By.XPATH, 
                                '//a[contains(@href, "ItemDetail")]'
                            )
                        
                        # å¯¹æ¯ä¸ªé“¾æ¥æ£€æŸ¥å…¶å¯¹åº”çš„å‘å¸ƒæ—¥æœŸï¼ˆåˆ©ç”¨å€’åºç‰¹æ€§ä¼˜åŒ–ï¼‰
                        target_date_found = False
                        for i, link in enumerate(punishment_links):
                            try:
                                href = link.get_attribute('href')
                                title = clean_text(link.text)
                                
                                if href and title:
                                    # è·å–è¯¥é“¾æ¥å¯¹åº”çš„å‘å¸ƒæ—¶é—´
                                    link_publish_date = self.get_link_publish_date(link)
                                    
                                    self.logger.debug(f"æ£€æŸ¥ç¬¬{i+1}æ¡è®°å½•: {title[:50]}... -> æ—¥æœŸ: {link_publish_date}")
                                    
                                    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨ç›®æ ‡æœˆä»½èŒƒå›´å†…
                                    if link_publish_date:
                                        # è§£ææ—¥æœŸ
                                        try:
                                            from datetime import datetime
                                            link_date = datetime.strptime(link_publish_date, '%Y-%m-%d')
                                            target_start = datetime(target_year, target_month, 1)
                                            
                                            # è®¡ç®—ç›®æ ‡æœˆä»½çš„ç»“æŸæ—¥æœŸ
                                            if target_month == 12:
                                                target_end = datetime(target_year + 1, 1, 1)
                                            else:
                                                target_end = datetime(target_year, target_month + 1, 1)
                                            
                                            if target_start <= link_date < target_end:
                                                # åœ¨ç›®æ ‡æœˆä»½èŒƒå›´å†…
                                                target_date_found = True
                                                
                                                # æ„å»ºå®Œæ•´URL
                                                if not href.startswith('http'):
                                                    href = urllib.parse.urljoin('https://www.nfra.gov.cn', href)
                                                
                                                punishment_info = {
                                                    'title': title,
                                                    'detail_url': href,
                                                    'category': category,
                                                    'page': current_page,
                                                    'publish_date': link_publish_date
                                                }
                                                page_punishment_list.append(punishment_info)
                                                self.logger.info(f"âœ“ æ‰¾åˆ°ç›®æ ‡è®°å½•: {title[:30]}... ({link_publish_date})")
                                                
                                            elif link_date < target_start:
                                                # æ—¥æœŸæ—©äºç›®æ ‡æœˆä»½ï¼Œç”±äºæ˜¯å€’åºæ’åˆ—ï¼Œåé¢çš„éƒ½ä¼šæ›´æ—©
                                                self.logger.info(f"âœ— å‘ç°æ—©äºç›®æ ‡æœˆä»½çš„è®°å½•: {link_publish_date}ï¼Œç”±äºå€’åºæ’åˆ—ï¼Œåœæ­¢æ£€æŸ¥å‰©ä½™ {len(punishment_links)-i-1} æ¡è®°å½•")
                                                break
                                            else:
                                                # æ—¥æœŸæ™šäºç›®æ ‡æœˆä»½ï¼Œç»§ç»­æ£€æŸ¥ä¸‹ä¸€æ¡
                                                self.logger.debug(f"â—‹ è·³è¿‡æ™šäºç›®æ ‡æœˆä»½çš„è®°å½•: {link_publish_date}")
                                                
                                        except ValueError as e:
                                            self.logger.warning(f"æ—¥æœŸè§£æå¤±è´¥: {link_publish_date} - {e}")
                                            continue
                                    else:
                                        self.logger.debug(f"æ— æ³•è·å–è®°å½•æ—¥æœŸ: {title[:30]}...")
                                        
                            except Exception as e:
                                self.logger.warning(f"è§£æç¬¬{i+1}æ¡é“¾æ¥å¤±è´¥: {e}")
                                continue
                        
                        target_date_count = len(page_punishment_list)
                        total_count = len(punishment_links)
                        self.logger.info(f"ç¬¬ {current_page} é¡µæ‰¾åˆ° {target_date_count} æ¡ç›®æ ‡æ—¥æœŸè®°å½• (å…±{total_count}æ¡)")
                        all_punishment_list.extend(page_punishment_list)
                        
                    except Exception as e:
                        self.logger.error(f"è§£æç¬¬ {current_page} é¡µå¤±è´¥: {e}")
                else:
                    # æ²¡æœ‰ç›®æ ‡æ—¥æœŸæ•°æ®
                    if found_target_date:
                        # ä¹‹å‰æ‰¾åˆ°è¿‡ç›®æ ‡æ—¥æœŸï¼Œç°åœ¨æ²¡æœ‰äº†ï¼Œè¯´æ˜å·²ç»è¶…è¿‡ç›®æ ‡æ—¥æœŸèŒƒå›´
                        self.logger.info(f"ç¬¬ {current_page} é¡µä¸å†åŒ…å«ç›®æ ‡æ—¥æœŸæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                        break
                    elif current_page == 1:
                        # ç¬¬1é¡µæ²¡æœ‰ç›®æ ‡æ—¥æœŸæ•°æ®ï¼Œç”±äºé¡µé¢æ˜¯é™åºæ’åˆ—ï¼Œåé¢é¡µé¢æ›´ä¸å¯èƒ½æœ‰
                        self.logger.info(f"ç¬¬1é¡µæš‚æ— ç›®æ ‡æ—¥æœŸ {target_date_str} æ•°æ®ï¼Œç”±äºé¡µé¢é™åºæ’åˆ—ï¼Œç›´æ¥è·³è¿‡è¯¥åˆ†ç±»")
                        break
                    else:
                        # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œå› ä¸ºç¬¬1é¡µæ²¡æœ‰æ•°æ®åº”è¯¥å·²ç»breakäº†
                        self.logger.info(f"ç¬¬ {current_page} é¡µæš‚æ— ç›®æ ‡æ—¥æœŸ {target_date_str} æ•°æ®")
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µå¹¶ä¸”è¿˜éœ€è¦ç»§ç»­
                if current_page < max_pages:
                    try:
                        next_buttons = self.driver.find_elements(
                            By.XPATH, 
                            '//span[text()="ä¸‹ä¸€é¡µ"] | //a[text()="ä¸‹ä¸€é¡µ"] | //a[contains(text(), "ä¸‹ä¸€é¡µ")]'
                        )
                        
                        if next_buttons:
                            next_button = next_buttons[0]
                            if next_button.is_enabled():
                                self.driver.execute_script("arguments[0].click();", next_button)
                                current_page += 1
                                time.sleep(1.5)  # å‡å°‘ç¿»é¡µç­‰å¾…æ—¶é—´ä»3ç§’åˆ°1.5ç§’
                                continue
                            else:
                                self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                        else:
                            self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                            break
                    except Exception as e:
                        self.logger.warning(f"ç¿»é¡µå¤±è´¥: {e}")
                        break
                else:
                    break
            
            # æ™ºèƒ½æ£€æŸ¥ç»“æœåé¦ˆ
            if not found_target_date:
                self.logger.info(f"{category} æ™ºèƒ½æ£€æŸ¥å®Œæˆï¼Œæœªæ‰¾åˆ° {target_date_str} çš„æ•°æ®")
            
            self.logger.info(f"{category} å¤„ç½šåˆ—è¡¨è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_punishment_list)} æ¡ç›®æ ‡æ—¥æœŸè®°å½•")
            return all_punishment_list
            
        except Exception as e:
            self.logger.error(f"è§£æ {category} å¤„ç½šåˆ—è¡¨å¤±è´¥: {e}")
            return all_punishment_list

    def get_link_publish_date(self, link_element) -> str:
        """è·å–é“¾æ¥å¯¹åº”çš„å‘å¸ƒæ—¥æœŸï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            import re
            
            # æ–¹æ³•1ï¼šä»è¡¨æ ¼è¡Œä¸­æŸ¥æ‰¾æ—¥æœŸï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            try:
                # æŸ¥æ‰¾åŒ…å«é“¾æ¥çš„è¡¨æ ¼è¡Œ
                row_element = link_element.find_element(By.XPATH, "./ancestor::tr[1]")
                
                # è·å–è¯¥è¡Œæ‰€æœ‰å•å…ƒæ ¼
                cells = row_element.find_elements(By.TAG_NAME, "td")
                
                # åœ¨æ¯ä¸ªå•å…ƒæ ¼ä¸­æŸ¥æ‰¾æ—¥æœŸ
                for cell in cells:
                    cell_text = clean_text(cell.text)
                    # åŒ¹é… YYYY-MM-DD æ ¼å¼çš„æ—¥æœŸ
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', cell_text)
                    if date_match:
                        found_date = date_match.group(1)
                        self.logger.debug(f"ä»è¡¨æ ¼å•å…ƒæ ¼æ‰¾åˆ°æ—¥æœŸ: {found_date}")
                        return found_date
                
                # å¦‚æœå•å…ƒæ ¼ä¸­æ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æ•´è¡Œçš„æ–‡æœ¬
                row_text = clean_text(row_element.text)
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', row_text)
                if date_match:
                    found_date = date_match.group(1)
                    self.logger.debug(f"ä»è¡¨æ ¼è¡Œæ–‡æœ¬æ‰¾åˆ°æ—¥æœŸ: {found_date}")
                    return found_date
                    
            except Exception as e:
                self.logger.debug(f"è¡¨æ ¼è¡ŒæŸ¥æ‰¾å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾é“¾æ¥å‘¨å›´çš„æ—¥æœŸä¿¡æ¯
            try:
                # å‘ä¸ŠæŸ¥æ‰¾çˆ¶å®¹å™¨ä¸­çš„æ—¥æœŸ
                for level in range(1, 6):  # å‘ä¸ŠæŸ¥æ‰¾5å±‚
                    parent = link_element
                    for _ in range(level):
                        parent = parent.find_element(By.XPATH, "..")
                    
                    # åœ¨çˆ¶å®¹å™¨ä¸­æŸ¥æ‰¾æ—¥æœŸ
                    parent_text = clean_text(parent.text)
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', parent_text)
                    if date_match:
                        found_date = date_match.group(1)
                        self.logger.debug(f"ä»ç¬¬{level}å±‚çˆ¶å…ƒç´ æ‰¾åˆ°æ—¥æœŸ: {found_date}")
                        return found_date
                        
            except Exception as e:
                self.logger.debug(f"çˆ¶å…ƒç´ æŸ¥æ‰¾å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾ç›¸é‚»å…ƒç´ ä¸­çš„æ—¥æœŸ
            try:
                # æŸ¥æ‰¾å‰åç›¸é‚»çš„å…ƒç´ 
                for xpath in ["./following-sibling::*[1]", "./preceding-sibling::*[1]"]:
                    sibling = link_element.find_element(By.XPATH, xpath)
                    sibling_text = clean_text(sibling.text)
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', sibling_text)
                    if date_match:
                        found_date = date_match.group(1)
                        self.logger.debug(f"ä»ç›¸é‚»å…ƒç´ æ‰¾åˆ°æ—¥æœŸ: {found_date}")
                        return found_date
                        
            except Exception as e:
                self.logger.debug(f"ç›¸é‚»å…ƒç´ æŸ¥æ‰¾å¤±è´¥: {e}")
            
            # æ–¹æ³•4ï¼šä»é¡µé¢å…¨å±€æ—¥æœŸåˆ—è¡¨ä¸­åŒ¹é…ï¼ˆæœ€åçš„å¤‡ç”¨æ–¹æ³•ï¼‰
            try:
                all_dates = self.get_page_publish_dates()
                if all_dates:
                    # å‡è®¾é“¾æ¥é¡ºåºä¸æ—¥æœŸé¡ºåºå¯¹åº”
                    all_links = self.driver.find_elements(
                        By.XPATH, 
                        '//a[contains(@href, "ItemDetail")]'
                    )
                    
                    # æ‰¾åˆ°å½“å‰é“¾æ¥åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®
                    link_index = -1
                    for i, link in enumerate(all_links):
                        if link == link_element:
                            link_index = i
                            break
                    
                    if 0 <= link_index < len(all_dates):
                        found_date = all_dates[link_index]
                        self.logger.debug(f"ä»é¡µé¢æ—¥æœŸåˆ—è¡¨åŒ¹é…åˆ°æ—¥æœŸ: {found_date}")
                        return found_date
                        
            except Exception as e:
                self.logger.debug(f"é¡µé¢æ—¥æœŸåˆ—è¡¨åŒ¹é…å¤±è´¥: {e}")
            
            self.logger.debug("æ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½è·å–é“¾æ¥çš„å‘å¸ƒæ—¥æœŸ")
            return ""
            
        except Exception as e:
            self.logger.warning(f"è·å–é“¾æ¥å‘å¸ƒæ—¥æœŸå¤±è´¥: {e}")
            return ""

    def crawl_selected_categories(self, categories: List[str], max_pages_per_category: int = 5, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """çˆ¬å–æŒ‡å®šç±»åˆ«çš„å¤„ç½šä¿¡æ¯"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        try:
            for category in categories:
                if category not in BASE_URLS:
                    self.logger.warning(f"è·³è¿‡æœªçŸ¥ç±»åˆ«: {category}")
                    continue
                    
                self.logger.info(f"å¼€å§‹çˆ¬å– {category}")
                category_data = self.crawl_category(category, max_pages_per_category, max_records_per_category)
                all_data[category] = category_data
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            self.logger.info("æŒ‡å®šç±»åˆ«çˆ¬å–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_selected_categories_by_month(self, categories: List[str], target_year: int, target_month: int, max_pages_per_category: int = 10, max_records_per_category: int = None, use_smart_check: bool = False) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«æŒ‡å®šæœˆä»½çš„æ‰€æœ‰å¤„ç½šä¿¡æ¯"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½æœˆä»½çˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡: {target_year}å¹´{target_month}æœˆ")
        self.logger.info(f"çˆ¬å–ç±»åˆ«: {', '.join(categories)}")
        
        try:
            for category in categories:
                if category not in BASE_URLS:
                    self.logger.warning(f"è·³è¿‡æœªçŸ¥ç±»åˆ«: {category}")
                    continue
                    
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} - {target_year}å¹´{target_month}æœˆæ•°æ®")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æŒ‡å®šæœˆä»½
                category_data = self.crawl_category_smart(
                    category, 
                    target_year, 
                    target_month, 
                    max_pages_per_category, 
                    max_records_per_category,
                    use_smart_check
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡{target_year}å¹´{target_month}æœˆè®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°{target_year}å¹´{target_month}æœˆçš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            if target_year and target_month:
                self.logger.info(f"æ™ºèƒ½çˆ¬å–å®Œæˆï¼Œ{target_year}å¹´{target_month}æœˆå…±è·å¾— {total_records} æ¡è®°å½•")
            else:
                self.logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_selected_categories_by_year(self, categories: List[str], target_year: int, max_pages_per_category: int = 50, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å¤„ç½šä¿¡æ¯"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½å¹´ä»½çˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡å¹´ä»½: {target_year}")
        self.logger.info(f"çˆ¬å–ç±»åˆ«: {', '.join(categories)}")
        
        try:
            for category in categories:
                if category not in BASE_URLS:
                    self.logger.warning(f"è·³è¿‡æœªçŸ¥ç±»åˆ«: {category}")
                    continue
                    
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} - {target_year}å¹´æ•°æ®")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æŒ‡å®šå¹´ä»½
                category_data = self.crawl_category_smart_by_year(
                    category, 
                    target_year, 
                    max_pages_per_category, 
                    max_records_per_category
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡{target_year}å¹´è®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°{target_year}å¹´çš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            self.logger.info(f"æ™ºèƒ½å¹´ä»½çˆ¬å–å®Œæˆï¼Œ{target_year}å¹´å…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data

    def crawl_selected_categories_by_date(self, categories: List[str], target_year: int, target_month: int, target_day: int, max_pages_per_category: int = 3, max_records_per_category: int = None) -> Dict[str, List[Dict]]:
        """æ™ºèƒ½çˆ¬å–æŒ‡å®šç±»åˆ«æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å¤„ç½šä¿¡æ¯"""
        if not self.setup_driver():
            self.logger.error("æ— æ³•åˆå§‹åŒ–WebDriverï¼Œçˆ¬å–å¤±è´¥")
            return {}
        
        all_data = {}
        target_date_str = f"{target_year}-{target_month:02d}-{target_day:02d}"
        
        self.logger.info(f"å¯ç”¨æ™ºèƒ½æ—¥æœŸçˆ¬å–æ¨¡å¼ï¼Œç›®æ ‡æ—¥æœŸ: {target_date_str}")
        self.logger.info(f"çˆ¬å–ç±»åˆ«: {', '.join(categories)}")
        
        try:
            for category in categories:
                if category not in BASE_URLS:
                    self.logger.warning(f"è·³è¿‡æœªçŸ¥ç±»åˆ«: {category}")
                    continue
                    
                self.logger.info(f"å¼€å§‹æ™ºèƒ½çˆ¬å– {category} - {target_date_str}æ•°æ®")
                
                # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æŒ‡å®šæ—¥æœŸ
                category_data = self.crawl_category_smart_by_date(
                    category, 
                    target_year, 
                    target_month, 
                    target_day,
                    max_pages_per_category, 
                    max_records_per_category
                )
                
                all_data[category] = category_data
                
                # è®°å½•è¿™ä¸ªåˆ†ç±»çš„ç»“æœ
                if category_data:
                    self.logger.info(f"{category} å®Œæˆï¼Œè·å¾— {len(category_data)} æ¡{target_date_str}è®°å½•")
                else:
                    self.logger.info(f"{category} å®Œæˆï¼Œæœªæ‰¾åˆ°{target_date_str}çš„æ•°æ®")
                
                # ç±»åˆ«é—´å»¶è¿Ÿ
                time.sleep(CRAWL_CONFIG['delay_between_requests'] * 2)
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_records = sum(len(records) for records in all_data.values())
            self.logger.info(f"æ™ºèƒ½æ—¥æœŸçˆ¬å–å®Œæˆï¼Œ{target_date_str}å…±è·å¾— {total_records} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ—¥æœŸçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.close_driver()
        
        return all_data


if __name__ == "__main__":
    crawler = NFRACrawler()
    data = crawler.crawl_all_smart()
    
    # è¾“å‡ºç»“æœç»Ÿè®¡
    for category, records in data.items():
        print(f"{category}: {len(records)} æ¡è®°å½•") 