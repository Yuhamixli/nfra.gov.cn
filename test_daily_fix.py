#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•dailyæ¨¡å¼çš„æ—¥æœŸè¿‡æ»¤ä¿®å¤
"""

import logging
from datetime import datetime, timedelta
from crawler import NFRACrawler

def test_daily_date_filter():
    """æµ‹è¯•dailyæ¨¡å¼çš„æ—¥æœŸè¿‡æ»¤åŠŸèƒ½"""
    
    # è®¾ç½®è¯¦ç»†æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 60)
    print("æµ‹è¯•Dailyæ¨¡å¼æ—¥æœŸè¿‡æ»¤ä¿®å¤")
    print("=" * 60)
    
    # æµ‹è¯•æ˜¨å¤©çš„æ—¥æœŸ
    yesterday = datetime.now() - timedelta(days=1)
    target_year, target_month, target_day = yesterday.year, yesterday.month, yesterday.day
    target_date_str = f"{target_year}-{target_month:02d}-{target_day:02d}"
    
    print(f"ç›®æ ‡æ—¥æœŸ: {target_date_str}")
    print(f"æµ‹è¯•ç±»åˆ«: ç›‘ç®¡å±€æœ¬çº§")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = NFRACrawler(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨ä¾¿äºè§‚å¯Ÿ
    
    if not crawler.setup_driver():
        print("æ— æ³•åˆå§‹åŒ–WebDriver")
        return
    
    try:
        print(f"\nå¼€å§‹æµ‹è¯•ç›‘ç®¡å±€æœ¬çº§ {target_date_str} æ•°æ®è¿‡æ»¤...")
        
        # ä½¿ç”¨ä¿®å¤åçš„æ–¹æ³•
        category_data = crawler.crawl_category_smart_by_date(
            category='ç›‘ç®¡å±€æœ¬çº§',
            target_year=target_year,
            target_month=target_month, 
            target_day=target_day,
            max_pages=2,  # é™åˆ¶2é¡µæµ‹è¯•
            max_records=5  # é™åˆ¶5æ¡è®°å½•æµ‹è¯•
        )
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   æ‰¾åˆ°ç›®æ ‡æ—¥æœŸè®°å½•æ•°: {len(category_data)}")
        
        if category_data:
            print(f"\nğŸ“‹ è®°å½•è¯¦æƒ…:")
            for i, record in enumerate(category_data, 1):
                print(f"   {i}. æ ‡é¢˜: {record.get('title', 'æœªçŸ¥')[:60]}...")
                print(f"      å‘å¸ƒæ—¥æœŸ: {record.get('publish_date', 'æœªçŸ¥')}")
                print(f"      è¯¦æƒ…é“¾æ¥: {record.get('detail_url', 'æœªçŸ¥')[:60]}...")
                print()
                
            # éªŒè¯æ‰€æœ‰è®°å½•éƒ½æ˜¯ç›®æ ‡æ—¥æœŸ
            all_target_date = all(
                record.get('publish_date', '').startswith(target_date_str) 
                for record in category_data
            )
            
            if all_target_date:
                print("âœ… æ‰€æœ‰è®°å½•éƒ½æ˜¯ç›®æ ‡æ—¥æœŸï¼Œè¿‡æ»¤åŠŸèƒ½æ­£å¸¸ï¼")
            else:
                print("âŒ å‘ç°éç›®æ ‡æ—¥æœŸè®°å½•ï¼Œè¿‡æ»¤åŠŸèƒ½å­˜åœ¨é—®é¢˜")
                non_target = [
                    record for record in category_data 
                    if not record.get('publish_date', '').startswith(target_date_str)
                ]
                print(f"   éç›®æ ‡æ—¥æœŸè®°å½•æ•°: {len(non_target)}")
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ°{target_date_str}çš„è®°å½•")
            print("   è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºè¯¥æ—¥æœŸç¡®å®æ²¡æœ‰æ•°æ®")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ”„ æ­£åœ¨å…³é—­WebDriver...")
        crawler.close_driver()
        print("âœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_daily_date_filter() 