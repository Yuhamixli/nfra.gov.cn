#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ€»å±€æœºå…³2025å¹´6æœˆæ•°æ®
"""

import time
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler import NFRACrawler
from utils import setup_logging

def test_zhongju_june_performance():
    """æµ‹è¯•æ€»å±€æœºå…³2025å¹´6æœˆæ•°æ®çš„æ€§èƒ½"""
    logger = setup_logging()
    
    print("=" * 60)
    print("ğŸš€ æ€§èƒ½æµ‹è¯•ï¼šæ€»å±€æœºå…³2025å¹´6æœˆæ•°æ®")
    print("=" * 60)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå¯ç”¨æ— å¤´æ¨¡å¼ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼‰
    crawler = NFRACrawler(headless=True)
    
    try:
        # è®¾ç½®ç›®æ ‡å‚æ•°
        target_year = 2025
        target_month = 6
        category = "æ€»å±€æœºå…³"
        
        print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
        print(f"   - ç›®æ ‡æ—¥æœŸ: {target_year}å¹´{target_month}æœˆ")
        print(f"   - æµ‹è¯•ç±»åˆ«: {category}")
        print(f"   - æ— å¤´æ¨¡å¼: å·²å¯ç”¨")
        print(f"   - æ™ºèƒ½è¿‡æ»¤: å·²å¯ç”¨")
        print()
        
        # åˆå§‹åŒ–WebDriver
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–WebDriver...")
        setup_start = time.time()
        if not crawler.setup_driver():
            print("âŒ WebDriveråˆå§‹åŒ–å¤±è´¥")
            return
        setup_time = time.time() - setup_start
        print(f"âœ… WebDriveråˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {setup_time:.2f}ç§’")
        print()
        
        # æ‰§è¡Œæ™ºèƒ½çˆ¬å–
        print("ğŸ•·ï¸ å¼€å§‹æ™ºèƒ½çˆ¬å–...")
        crawl_start = time.time()
        
        # ä½¿ç”¨æ™ºèƒ½æ–¹æ³•çˆ¬å–æ€»å±€æœºå…³2025å¹´6æœˆæ•°æ®
        category_data = crawler.crawl_category_smart(
            category=category,
            target_year=target_year,
            target_month=target_month,
            max_pages=5,  # é™åˆ¶æœ€å¤§é¡µæ•°
            max_records=10,  # é™åˆ¶æœ€å¤§è®°å½•æ•°ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
            use_smart_check=True  # å¯ç”¨æ™ºèƒ½æ£€æŸ¥
        )
        
        crawl_time = time.time() - crawl_start
        print(f"âœ… æ™ºèƒ½çˆ¬å–å®Œæˆï¼Œè€—æ—¶: {crawl_time:.2f}ç§’")
        print()
        
        # ç»Ÿè®¡ç»“æœ
        print("ğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"   - æ‰¾åˆ°è®°å½•æ•°: {len(category_data)}")
        print(f"   - çˆ¬å–è€—æ—¶: {crawl_time:.2f}ç§’")
        print(f"   - åˆå§‹åŒ–è€—æ—¶: {setup_time:.2f}ç§’")
        print(f"   - æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
        print()
        
        # æ˜¾ç¤ºæ‰¾åˆ°çš„è®°å½•
        if category_data:
            print("ğŸ“‹ æ‰¾åˆ°çš„è®°å½•:")
            for i, record in enumerate(category_data, 1):
                title = record.get('title', 'æœªçŸ¥æ ‡é¢˜')
                publish_date = record.get('publish_date', 'æœªçŸ¥æ—¥æœŸ')
                print(f"   {i}. {title[:50]}... ({publish_date})")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®°å½•")
        
        print()
        print("ğŸ¯ æ€§èƒ½åˆ†æ:")
        if crawl_time < 120:  # å°‘äº2åˆ†é’Ÿ
            print("   âœ… æ€§èƒ½ä¼˜ç§€ï¼šçˆ¬å–é€Ÿåº¦å¿«äºé¢„æœŸ")
        elif crawl_time < 180:  # å°‘äº3åˆ†é’Ÿ
            print("   âœ… æ€§èƒ½è‰¯å¥½ï¼šçˆ¬å–é€Ÿåº¦ç¬¦åˆé¢„æœŸ")
        else:
            print("   âš ï¸ æ€§èƒ½ä¸€èˆ¬ï¼šå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
        avg_time_per_record = crawl_time / max(len(category_data), 1)
        print(f"   - å¹³å‡æ¯æ¡è®°å½•å¤„ç†æ—¶é—´: {avg_time_per_record:.2f}ç§’")
        
        return category_data
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        # å…³é—­WebDriver
        if crawler.driver:
            crawler.close_driver()
            print("ğŸ”§ WebDriverå·²å…³é—­")

if __name__ == "__main__":
    test_zhongju_june_performance() 